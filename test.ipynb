{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import CapsuleRouting\n",
    "import torch\n",
    "routing = CapsuleRouting(mode='em')\n",
    "\n",
    "v = torch.rand(2, 8, 10, 16, 1, 1)\n",
    "a = torch.rand(2, 8, 1, 1)\n",
    "v, a = routing(v, a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import AdaptiveCapsuleHead\n",
    "import torch\n",
    "\n",
    "model = AdaptiveCapsuleHead(32, 10, 4, 1, False, *[\"dynamic\", 10, 1.5])\n",
    "v = torch.rand(2, 32, 5, 5)\n",
    "\n",
    "out, v = model(v, get_capsules=True)\n",
    "print(v.shape)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from Capsule.Layer.Classifier import CapsuleWrappingClassifier\n",
    "with open('config/new-config.yml', 'r') as stream:\n",
    "    PARAMS = yaml.safe_load(stream)\n",
    "    print(PARAMS)\n",
    "model = CapsuleWrappingClassifier(model_configs=PARAMS['architect_settings'])\n",
    "\n",
    "img = torch.rand(2, 3, 224, 224)\n",
    "\n",
    "out = model(img)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.ultis import Cub2011\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "dataset = Cub2011(mode='test', data_path='data/CUB_200_2011', transform=None)\n",
    "\n",
    "img, label, oimg = dataset[10]\n",
    "print(label)\n",
    "plt.imshow(ToPILImage()(oimg))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.arange(60.).reshape(3,4,5)\n",
    "b = np.arange(24.).reshape(4,3,2)\n",
    "print(a)\n",
    "print(b)\n",
    "np.einsum('ijk,jil->kl', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "A = torch.rand(2, 15, 4, 4, 3, 3)\n",
    "B = torch.rand(15, 4, 4, 8)\n",
    "v = torch.einsum('bBijHW, BjkC -> bBCikHW', A, B)\n",
    "print(v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS =  {\n",
    "    \"architect_settings\" : {\n",
    "            \"task\": \"None\",\n",
    "            \"name\": \"model-test\",\n",
    "            \"backbone\": {\n",
    "                    \"name\": \"resnet-s\",\n",
    "                    \"is_full\": False,\n",
    "                    \"is_pretrained\": True,\n",
    "                    \"is_feats\": False,\n",
    "                    \"is_freeze\": True, \n",
    "            },\n",
    "            \"n_cls\": 2,\n",
    "            \"is_caps\": False,\n",
    "            \"caps\":{\n",
    "                \"mode\": 1,\n",
    "                \"cap_dims\": 4,\n",
    "                \"routing\":{\n",
    "                        \"type\": \"dynamic\",\n",
    "                        \"params\": [3, 0.01, 1.5]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "    \"dataset_settings\": {\n",
    "            \n",
    "            },\n",
    "    \"training_settings\":{\n",
    "    \n",
    "    }\n",
    "}\n",
    "\n",
    "from Capsule.Layer.Classifier import CapsuleWrappingClassifier\n",
    "from Capsule.ultis import Cub2011, CIFAR10read\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "model = CapsuleWrappingClassifier(model_configs=PARAMS[\"architect_settings\"])\n",
    "model.eval()\n",
    "\n",
    "def get_train_data(model, dataloder):\n",
    "    train_data = []\n",
    "    train_label = []    \n",
    "    for batch in dataloder:\n",
    "        img, label, oimg = batch\n",
    "        feats = model.backbone(img)\n",
    "        out = feats.reshape(-1, 512, 7, 7)\n",
    "        train_data.append(out.detach().numpy())\n",
    "        train_label.append(label.detach().numpy())\n",
    "\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    train_label = np.concatenate(train_label, axis=0)\n",
    "    print(train_data.shape)\n",
    "    print(train_label.shape)\n",
    "    return train_data, train_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10read(mode=\"train\", data_path=\"data\")\n",
    "dataloder = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "train_data, train_label = get_train_data(model, dataloder)\n",
    "\n",
    "hf = h5py.File('data/CIFAR10_train_data.h5', 'w')\n",
    "hf.create_dataset('data', data=train_data)\n",
    "hf.create_dataset('label', data=train_label)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10read(mode=\"test\", data_path=\"data\")\n",
    "dataloder = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "test_data, test_label = get_train_data(model, dataloder)\n",
    "\n",
    "hf = h5py.File('data/CIFAR10_test_data.h5', 'w')\n",
    "hf.create_dataset('data', data=test_data)\n",
    "hf.create_dataset('label', data=test_label)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import h5py\n",
    "\n",
    "hf = h5py.File('data/CIFAR10_train_data.h5', 'r')\n",
    "train_data = np.array(hf['data'])\n",
    "train_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "hf = h5py.File('data/CIFAR10_test_data.h5', 'r')\n",
    "test_data = np.array(hf['data'])\n",
    "test_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "\n",
    "train_data = np.mean(train_data, axis=(2, 3))\n",
    "test_data = np.mean(test_data, axis=(2, 3))\n",
    "\n",
    "#scikit-learn MLPClassifier on train_data, train_label\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(512, 512), max_iter=1000)\n",
    "clf.fit(train_data, train_label)\n",
    "accuracy_score(clf.predict(test_data), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(train_data, train_label)\n",
    "accuracy_score(clf.predict(train_data), train_label)\n",
    "accuracy_score(clf.predict(test_data), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hf = h5py.File('data/CUB_ResNet_train_data.h5', 'r')\n",
    "train_data = np.array(hf['data'])\n",
    "train_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "\n",
    "tensor_x = torch.Tensor(train_data) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(train_label)\n",
    "my_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "train_loader = DataLoader(my_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('data/CUB_ResNet_test_data.h5', 'r')\n",
    "test_data = np.array(hf['data'])\n",
    "test_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "\n",
    "tensor_x = torch.Tensor(test_data) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(test_label)\n",
    "my_val_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "valid_loader = DataLoader(my_val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from Capsule.CapsuleLayer import AdaptiveCapsuleHead\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "                AdaptiveCapsuleHead(512, 200,\n",
    "                4, 4, True, \n",
    "                *['dynamic', 3, 2]),\n",
    "                # nn.LogSoftmax(dim=1),\n",
    "                nn.Flatten(start_dim=1)\n",
    "            )\n",
    "\n",
    "# Define the classifier on top of the ResNet\n",
    "# classifier = nn.Sequential(\n",
    "#     nn.Linear(512, 512, 1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(512, 512, 1),\n",
    "#     # nn.LogSoftmax(dim=1),\n",
    "#     # nn.Flatten(start_dim=1)\n",
    "# )\n",
    "\n",
    "# # Combine the ResNet and classifier\n",
    "# from torchvision.models import resnet18\n",
    "# resnet = resnet18(pretrained=True)\n",
    "# resnet.fc = nn.Identity()\n",
    "# model = nn.Sequential(resnet, classifier)\n",
    "model = classifier.to(device)\n",
    "# Load the MNIST dataset\n",
    "# train_dataset = Cub2011(mode=\"train\", data_path=\"data/CUB_200_2011\")\n",
    "# valid_dataset = Cub2011(mode=\"val\", data_path=\"data/CUB_200_2011\")\n",
    "\n",
    "# # Set hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "# # Create data loaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "max_val = 0\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/ {num_epochs} - Training\"):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        # print(images, labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Compute average training accuracy and loss\n",
    "    train_accuracy = 100.0 * train_correct / total_train_samples\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_correct = 0\n",
    "    total_valid_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(valid_loader, desc=f\"Epoch {epoch + 1}/ {num_epochs} - Validation\"):\n",
    "            # Forward pass\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            valid_correct += (predicted == labels).sum().item()\n",
    "            total_valid_samples += labels.size(0)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    # Compute average validation accuracy and loss\n",
    "    valid_accuracy = 100.0 * valid_correct / total_valid_samples\n",
    "    valid_loss /= len(valid_loader)\n",
    "    if(valid_accuracy > max_val):\n",
    "        max_val = valid_accuracy\n",
    "    # Print epoch-wise results\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Valid Loss: {valid_loss:.4f} | Valid Accuracy: {valid_accuracy:.2f}%\")\n",
    "print(f\"Max Valid Accuracy: {max_val:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUB\n",
    "- CUB-resnet-sci-SVM: 0.5904\n",
    "- CUB-resnet-freeze-2hMLP: 0.5856\n",
    "- CUB-resnet-sci-MLP: 0.577\n",
    "\n",
    "CIFAR\n",
    "- CIFAR-resnet-sci-MLP: 0.8717\n",
    "- CIFAR-resnet-sci-SVM: 0.8822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import CapsuleRouting\n",
    "import torch\n",
    "p = torch.randn(2, 32, 16, 4, 4)\n",
    "a = torch.randn(2, 32, 4, 4)\n",
    "params = {\"reduce\": True,\n",
    "        \"cap_dims\": 4,\n",
    "        \"routing\":{\n",
    "                \"type\": \"em\",\n",
    "                \"iters\": 3,\n",
    "                \"temp\": 1.5,\n",
    "        }\n",
    "}\n",
    "\n",
    "dynamic_routing = CapsuleRouting(B=32, C=10, caps=params)\n",
    "dynamic_routing.eval()\n",
    "v_out, a_out = dynamic_routing(p, a)\n",
    "print(v_out.shape, a_out.shape)\n",
    "print(a_out, v_out)\n",
    "# print(dynamic_routing.W_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import AdaptiveCapsuleHead\n",
    "import torch\n",
    "p = torch.randn(2, 512, 7, 7)\n",
    "# a = torch.randn(2, 512, 7, 7)\n",
    "params = {\n",
    "    \"n_cls\": 10,\n",
    "    \"n_layers\": 2,\n",
    "    \"n_emb\": 512,\n",
    "    \"is_caps\": False,\n",
    "    \"caps\": {\n",
    "        \"pooling\": \"None\",\n",
    "        \"shuffle\": True,\n",
    "        \"cap_style\": \"c\", # c: by channel, hw: by height and width\n",
    "        \"cap_dims\": 4,\n",
    "        \"routing\":{\n",
    "            \"reduce\": True,\n",
    "            \"type\": \"dynamic\",\n",
    "            \"iters\": 3,\n",
    "            \"temp\": 1.5,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "adaptive_capsule_head = AdaptiveCapsuleHead(512, head=params)\n",
    "adaptive_capsule_head.eval()\n",
    "a_out = adaptive_capsule_head(p)\n",
    "print(a_out.shape)\n",
    "print(a_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import ProjectionHead\n",
    "import torch\n",
    "p = torch.randn(2, 512, 7, 7)\n",
    "\n",
    "projection_head = ProjectionHead(512, 32, 1)\n",
    "print(projection_head)\n",
    "out = projection_head(p)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptivePoolCapsuleHead(nn.Module):\n",
    "    '''\n",
    "    Capsule Header combines of Primary Capsule and Linear Capsule.\n",
    "    \n",
    "    - Arguments:\n",
    "        B: number of capsule in L layer\n",
    "        C: number of capsule in L + 1 layer\n",
    "        get_capsules: Return Capsules's vector\n",
    "        P: size of a capsule\n",
    "        reduce: reduce the featrue maps before routing\n",
    "        args: arguments for routing method\n",
    "        argv[0]: routing method\n",
    "        argv[1]: number of iteration\n",
    "        argv[2]: m for fuzzy routing\n",
    "    '''\n",
    "    def __init__(self, B, head):\n",
    "        super(AdaptivePoolCapsuleHead, self).__init__()\n",
    "     \n",
    "  \n",
    "        self.reduce = head['caps']['reduce']\n",
    "        self.n_layers = head['n_layers']\n",
    "        n_emb = head['n_emb']\n",
    "        self.P = head['caps']['cap_dims']\n",
    "\n",
    "        assert B % (self.P * self.P) == 0, \"channel is not divisible by P * P\"\n",
    "        self.B = B // (self.P * self.P)\n",
    "        assert n_emb % (self.P * self.P) == 0, \"embedding is not divisible by P * P\"\n",
    "        self.n_emb = n_emb // (self.P * self.P)\n",
    "      \n",
    "        self.primary_capsule = nn.Sequential()\n",
    "        self.a_routing = nn.Sequential()\n",
    "        if(self.reduce):\n",
    "            self.primary_capsule.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "            self.a_routing.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        if(self.n_layers == 1):\n",
    "            self.primary_capsule.append(nn.Identity())\n",
    "            self.routinglayer = CapsuleRouting(self.B, head['n_cls'], head['caps'])\n",
    "        else:\n",
    "            self.primary_capsule.append(nn.Conv2d(B, n_emb, 1))\n",
    "            self.primary_capsule.append(nn.ReLU())\n",
    "            for i in range(1, self.n_layers - 1):\n",
    "                self.primary_capsule.append(nn.Conv2d(n_emb, n_emb, 1))\n",
    "                if(i < self.n_layers - 1):\n",
    "                    self.primary_capsule.append(nn.ReLU())\n",
    "            self.routinglayer = CapsuleRouting(self.n_emb, head['n_cls'], head['caps'])\n",
    "\n",
    "        self.a_routing.append(nn.Conv2d(B, self.B, 1))\n",
    "\n",
    "    def forward(self, x, get_capsules=False):\n",
    "        '''\n",
    "        input: \n",
    "            tensor 4D (b, B, h, w)\n",
    "        output:\n",
    "            capsule 3D (b, C, P*P) / 5D (b, C, P*P, h, w)\n",
    "            activation 2D (b, C) / 5D (b, C, h, w)\n",
    "        '''\n",
    "        # Primary capsule\n",
    "        \n",
    "        # p <- (b, B, P * P, h, w)\n",
    "        # a <- (b, B, h, w)\n",
    "        p = self.primary_capsule(x)\n",
    "        # x <- (b, C, h, w)\n",
    "        b, d, h, w =  p.shape\n",
    "        p = p.reshape(b, self.B , self.P ** 2, h, w)\n",
    "       \n",
    "        a = self.a_routing(x)\n",
    "        a = torch.sigmoid(a)\n",
    "        \n",
    "        print(p.shape, a.shape)\n",
    "        p_out, a_out = self.routinglayer(p, a)\n",
    "        a_out = torch.log(a_out / (1 - a_out + EPS))\n",
    "       \n",
    "        if get_capsules:\n",
    "            return p_out, a_out\n",
    "        else: \n",
    "            return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "orthogonal_weight = torch.nn.utils.parametrizations.orthogonal(nn.Conv2d(2, 2, 3))\n",
    "print(orthogonal_weight.weight[0][0].T @ orthogonal_weight.weight[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.model import CapsuleWrappingClassifier, CapsuleWrappingSegment\n",
    "import yaml\n",
    "with open('config/new-config.yml', 'r') as stream:\n",
    "        PARAMS = yaml.safe_load(stream)\n",
    "        print(PARAMS)\n",
    "\n",
    "model = CapsuleWrappingSegment(PARAMS['architect_settings'])\n",
    "\n",
    "import torch\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "out = model(x)\n",
    "# print(model)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.ultis import CIFAR100read\n",
    "\n",
    "dataset = CIFAR100read(mode='train', data_path='data')\n",
    "\n",
    "print(dataset[0][0].shape, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Caltech101\n",
    "dataset = Caltech101(root='data', download=True)\n",
    "dataset[6999][0]\n",
    "# print(dataset[0][0].shape, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.ultis import Caltech101read\n",
    "from torchvision.transforms import ToPILImage\n",
    "dataset = Caltech101read(mode='train', data_path='data')\n",
    "ToPILImage()(dataset[1000][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import VOCSegmentation\n",
    "dataset = VOCSegmentation(root='data', download=False, image_set='train', transforms=None)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1449"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import VOCSegmentation\n",
    "import numpy as np\n",
    "dataset = VOCSegmentation(root='data', download=False, image_set='val', transforms=None)\n",
    "len(dataset)\n",
    "# dataset[10][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Capsule.ultis import VOC2012read\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# import albumentations as A\n",
    "import cv2\n",
    "\n",
    "# transform = A.Compose([\n",
    "#     A.RandomCrop(width=256, height=256),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.RandomBrightnessContrast(p=0.2),\n",
    "# ])\n",
    "\n",
    "dataset = VOC2012read(mode='val', data_path='data')\n",
    "ToPILImage()(dataset[30][1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capsule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
