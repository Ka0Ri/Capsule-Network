{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import CapsuleRouting\n",
    "import torch\n",
    "routing = CapsuleRouting(mode='em')\n",
    "\n",
    "v = torch.rand(2, 8, 10, 16, 1, 1)\n",
    "a = torch.rand(2, 8, 1, 1)\n",
    "v, a = routing(v, a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import AdaptiveCapsuleHead\n",
    "import torch\n",
    "\n",
    "model = AdaptiveCapsuleHead(32, 10, 4, 1, False, *[\"dynamic\", 10, 1.5])\n",
    "v = torch.rand(2, 32, 5, 5)\n",
    "\n",
    "out, v = model(v, get_capsules=True)\n",
    "print(v.shape)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from Capsule.Layer.Classifier import CapsuleWrappingClassifier\n",
    "with open('config/new-config.yml', 'r') as stream:\n",
    "    PARAMS = yaml.safe_load(stream)\n",
    "    print(PARAMS)\n",
    "model = CapsuleWrappingClassifier(model_configs=PARAMS['architect_settings'])\n",
    "\n",
    "img = torch.rand(2, 3, 224, 224)\n",
    "\n",
    "out = model(img)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.ultis import Cub2011\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "dataset = Cub2011(mode='test', data_path='data/CUB_200_2011', transform=None)\n",
    "\n",
    "img, label, oimg = dataset[10]\n",
    "print(label)\n",
    "plt.imshow(ToPILImage()(oimg))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.arange(60.).reshape(3,4,5)\n",
    "b = np.arange(24.).reshape(4,3,2)\n",
    "print(a)\n",
    "print(b)\n",
    "np.einsum('ijk,jil->kl', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "A = torch.rand(2, 15, 4, 4, 3, 3)\n",
    "B = torch.rand(15, 4, 4, 8)\n",
    "v = torch.einsum('bBijHW, BjkC -> bBCikHW', A, B)\n",
    "print(v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS =  {\n",
    "    \"architect_settings\" : {\n",
    "            \"task\": \"None\",\n",
    "            \"name\": \"model-test\",\n",
    "            \"backbone\": {\n",
    "                    \"name\": \"resnet-s\",\n",
    "                    \"is_full\": False,\n",
    "                    \"is_pretrained\": True,\n",
    "                    \"is_feats\": False,\n",
    "                    \"is_freeze\": True, \n",
    "            },\n",
    "            \"n_cls\": 2,\n",
    "            \"is_caps\": False,\n",
    "            \"caps\":{\n",
    "                \"mode\": 1,\n",
    "                \"cap_dims\": 4,\n",
    "                \"routing\":{\n",
    "                        \"type\": \"dynamic\",\n",
    "                        \"params\": [3, 0.01, 1.5]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "    \"dataset_settings\": {\n",
    "            \n",
    "            },\n",
    "    \"training_settings\":{\n",
    "    \n",
    "    }\n",
    "}\n",
    "\n",
    "from Capsule.Layer.Classifier import CapsuleWrappingClassifier\n",
    "from Capsule.ultis import Cub2011, CIFAR10read\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "model = CapsuleWrappingClassifier(model_configs=PARAMS[\"architect_settings\"])\n",
    "model.eval()\n",
    "\n",
    "def get_train_data(model, dataloder):\n",
    "    train_data = []\n",
    "    train_label = []    \n",
    "    for batch in dataloder:\n",
    "        img, label, oimg = batch\n",
    "        feats = model.backbone(img)\n",
    "        out = feats.reshape(-1, 512, 7, 7)\n",
    "        train_data.append(out.detach().numpy())\n",
    "        train_label.append(label.detach().numpy())\n",
    "\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    train_label = np.concatenate(train_label, axis=0)\n",
    "    print(train_data.shape)\n",
    "    print(train_label.shape)\n",
    "    return train_data, train_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10read(mode=\"train\", data_path=\"data\")\n",
    "dataloder = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "train_data, train_label = get_train_data(model, dataloder)\n",
    "\n",
    "hf = h5py.File('data/CIFAR10_train_data.h5', 'w')\n",
    "hf.create_dataset('data', data=train_data)\n",
    "hf.create_dataset('label', data=train_label)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10read(mode=\"test\", data_path=\"data\")\n",
    "dataloder = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "test_data, test_label = get_train_data(model, dataloder)\n",
    "\n",
    "hf = h5py.File('data/CIFAR10_test_data.h5', 'w')\n",
    "hf.create_dataset('data', data=test_data)\n",
    "hf.create_dataset('label', data=test_label)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import h5py\n",
    "\n",
    "hf = h5py.File('data/CIFAR10_train_data.h5', 'r')\n",
    "train_data = np.array(hf['data'])\n",
    "train_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "hf = h5py.File('data/CIFAR10_test_data.h5', 'r')\n",
    "test_data = np.array(hf['data'])\n",
    "test_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "\n",
    "train_data = np.mean(train_data, axis=(2, 3))\n",
    "test_data = np.mean(test_data, axis=(2, 3))\n",
    "\n",
    "#scikit-learn MLPClassifier on train_data, train_label\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(512, 512), max_iter=1000)\n",
    "clf.fit(train_data, train_label)\n",
    "accuracy_score(clf.predict(test_data), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(train_data, train_label)\n",
    "accuracy_score(clf.predict(train_data), train_label)\n",
    "accuracy_score(clf.predict(test_data), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hf = h5py.File('data/CUB_ResNet_train_data.h5', 'r')\n",
    "train_data = np.array(hf['data'])\n",
    "train_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "\n",
    "tensor_x = torch.Tensor(train_data) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(train_label)\n",
    "my_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "train_loader = DataLoader(my_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('data/CUB_ResNet_test_data.h5', 'r')\n",
    "test_data = np.array(hf['data'])\n",
    "test_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "\n",
    "tensor_x = torch.Tensor(test_data) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(test_label)\n",
    "my_val_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "valid_loader = DataLoader(my_val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from Capsule.CapsuleLayer import AdaptiveCapsuleHead\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "                AdaptiveCapsuleHead(512, 200,\n",
    "                4, 4, True, \n",
    "                *['dynamic', 3, 2]),\n",
    "                # nn.LogSoftmax(dim=1),\n",
    "                nn.Flatten(start_dim=1)\n",
    "            )\n",
    "\n",
    "# Define the classifier on top of the ResNet\n",
    "# classifier = nn.Sequential(\n",
    "#     nn.Linear(512, 512, 1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(512, 512, 1),\n",
    "#     # nn.LogSoftmax(dim=1),\n",
    "#     # nn.Flatten(start_dim=1)\n",
    "# )\n",
    "\n",
    "# # Combine the ResNet and classifier\n",
    "# from torchvision.models import resnet18\n",
    "# resnet = resnet18(pretrained=True)\n",
    "# resnet.fc = nn.Identity()\n",
    "# model = nn.Sequential(resnet, classifier)\n",
    "model = classifier.to(device)\n",
    "# Load the MNIST dataset\n",
    "# train_dataset = Cub2011(mode=\"train\", data_path=\"data/CUB_200_2011\")\n",
    "# valid_dataset = Cub2011(mode=\"val\", data_path=\"data/CUB_200_2011\")\n",
    "\n",
    "# # Set hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "# # Create data loaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "max_val = 0\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/ {num_epochs} - Training\"):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        # print(images, labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Compute average training accuracy and loss\n",
    "    train_accuracy = 100.0 * train_correct / total_train_samples\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_correct = 0\n",
    "    total_valid_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(valid_loader, desc=f\"Epoch {epoch + 1}/ {num_epochs} - Validation\"):\n",
    "            # Forward pass\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            valid_correct += (predicted == labels).sum().item()\n",
    "            total_valid_samples += labels.size(0)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    # Compute average validation accuracy and loss\n",
    "    valid_accuracy = 100.0 * valid_correct / total_valid_samples\n",
    "    valid_loss /= len(valid_loader)\n",
    "    if(valid_accuracy > max_val):\n",
    "        max_val = valid_accuracy\n",
    "    # Print epoch-wise results\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Valid Loss: {valid_loss:.4f} | Valid Accuracy: {valid_accuracy:.2f}%\")\n",
    "print(f\"Max Valid Accuracy: {max_val:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUB\n",
    "- CUB-resnet-sci-SVM: 0.5904\n",
    "- CUB-resnet-freeze-2hMLP: 0.5856\n",
    "- CUB-resnet-sci-MLP: 0.577\n",
    "\n",
    "CIFAR\n",
    "- CIFAR-resnet-sci-MLP: 0.8717\n",
    "- CIFAR-resnet-sci-SVM: 0.8822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import CapsuleRouting\n",
    "import torch\n",
    "p = torch.randn(2, 32, 16, 4, 4)\n",
    "a = torch.randn(2, 32, 4, 4)\n",
    "params = {\"reduce\": True,\n",
    "        \"cap_dims\": 4,\n",
    "        \"routing\":{\n",
    "                \"type\": \"em\",\n",
    "                \"iters\": 3,\n",
    "                \"temp\": 1.5,\n",
    "        }\n",
    "}\n",
    "\n",
    "dynamic_routing = CapsuleRouting(B=32, C=10, caps=params)\n",
    "dynamic_routing.eval()\n",
    "v_out, a_out = dynamic_routing(p, a)\n",
    "print(v_out.shape, a_out.shape)\n",
    "print(a_out, v_out)\n",
    "# print(dynamic_routing.W_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import AdaptiveCapsuleHead\n",
    "import torch\n",
    "p = torch.randn(2, 512, 7, 7)\n",
    "# a = torch.randn(2, 512, 7, 7)\n",
    "params = {\n",
    "    \"n_cls\": 10,\n",
    "    \"n_layers\": 2,\n",
    "    \"n_emb\": 512,\n",
    "    \"is_caps\": False,\n",
    "    \"caps\": {\n",
    "        \"pooling\": \"None\",\n",
    "        \"shuffle\": True,\n",
    "        \"cap_style\": \"c\", # c: by channel, hw: by height and width\n",
    "        \"cap_dims\": 4,\n",
    "        \"routing\":{\n",
    "            \"reduce\": True,\n",
    "            \"type\": \"dynamic\",\n",
    "            \"iters\": 3,\n",
    "            \"temp\": 1.5,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "adaptive_capsule_head = AdaptiveCapsuleHead(512, head=params)\n",
    "adaptive_capsule_head.eval()\n",
    "a_out = adaptive_capsule_head(p)\n",
    "print(a_out.shape)\n",
    "print(a_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import ProjectionHead\n",
    "import torch\n",
    "p = torch.randn(2, 512, 7, 7)\n",
    "\n",
    "projection_head = ProjectionHead(512, 32, 1)\n",
    "print(projection_head)\n",
    "out = projection_head(p)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptivePoolCapsuleHead(nn.Module):\n",
    "    '''\n",
    "    Capsule Header combines of Primary Capsule and Linear Capsule.\n",
    "    \n",
    "    - Arguments:\n",
    "        B: number of capsule in L layer\n",
    "        C: number of capsule in L + 1 layer\n",
    "        get_capsules: Return Capsules's vector\n",
    "        P: size of a capsule\n",
    "        reduce: reduce the featrue maps before routing\n",
    "        args: arguments for routing method\n",
    "        argv[0]: routing method\n",
    "        argv[1]: number of iteration\n",
    "        argv[2]: m for fuzzy routing\n",
    "    '''\n",
    "    def __init__(self, B, head):\n",
    "        super(AdaptivePoolCapsuleHead, self).__init__()\n",
    "     \n",
    "  \n",
    "        self.reduce = head['caps']['reduce']\n",
    "        self.n_layers = head['n_layers']\n",
    "        n_emb = head['n_emb']\n",
    "        self.P = head['caps']['cap_dims']\n",
    "\n",
    "        assert B % (self.P * self.P) == 0, \"channel is not divisible by P * P\"\n",
    "        self.B = B // (self.P * self.P)\n",
    "        assert n_emb % (self.P * self.P) == 0, \"embedding is not divisible by P * P\"\n",
    "        self.n_emb = n_emb // (self.P * self.P)\n",
    "      \n",
    "        self.primary_capsule = nn.Sequential()\n",
    "        self.a_routing = nn.Sequential()\n",
    "        if(self.reduce):\n",
    "            self.primary_capsule.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "            self.a_routing.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        if(self.n_layers == 1):\n",
    "            self.primary_capsule.append(nn.Identity())\n",
    "            self.routinglayer = CapsuleRouting(self.B, head['n_cls'], head['caps'])\n",
    "        else:\n",
    "            self.primary_capsule.append(nn.Conv2d(B, n_emb, 1))\n",
    "            self.primary_capsule.append(nn.ReLU())\n",
    "            for i in range(1, self.n_layers - 1):\n",
    "                self.primary_capsule.append(nn.Conv2d(n_emb, n_emb, 1))\n",
    "                if(i < self.n_layers - 1):\n",
    "                    self.primary_capsule.append(nn.ReLU())\n",
    "            self.routinglayer = CapsuleRouting(self.n_emb, head['n_cls'], head['caps'])\n",
    "\n",
    "        self.a_routing.append(nn.Conv2d(B, self.B, 1))\n",
    "\n",
    "    def forward(self, x, get_capsules=False):\n",
    "        '''\n",
    "        input: \n",
    "            tensor 4D (b, B, h, w)\n",
    "        output:\n",
    "            capsule 3D (b, C, P*P) / 5D (b, C, P*P, h, w)\n",
    "            activation 2D (b, C) / 5D (b, C, h, w)\n",
    "        '''\n",
    "        # Primary capsule\n",
    "        \n",
    "        # p <- (b, B, P * P, h, w)\n",
    "        # a <- (b, B, h, w)\n",
    "        p = self.primary_capsule(x)\n",
    "        # x <- (b, C, h, w)\n",
    "        b, d, h, w =  p.shape\n",
    "        p = p.reshape(b, self.B , self.P ** 2, h, w)\n",
    "       \n",
    "        a = self.a_routing(x)\n",
    "        a = torch.sigmoid(a)\n",
    "        \n",
    "        print(p.shape, a.shape)\n",
    "        p_out, a_out = self.routinglayer(p, a)\n",
    "        a_out = torch.log(a_out / (1 - a_out + EPS))\n",
    "       \n",
    "        if get_capsules:\n",
    "            return p_out, a_out\n",
    "        else: \n",
    "            return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "orthogonal_weight = torch.nn.utils.parametrizations.orthogonal(nn.Conv2d(2, 2, 3))\n",
    "print(orthogonal_weight.weight[0][0].T @ orthogonal_weight.weight[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.model import CapsuleWrappingClassifier, CapsuleWrappingSegment\n",
    "import yaml\n",
    "with open('config/new-config.yml', 'r') as stream:\n",
    "        PARAMS = yaml.safe_load(stream)\n",
    "        print(PARAMS)\n",
    "\n",
    "model = CapsuleWrappingSegment(PARAMS['architect_settings'])\n",
    "\n",
    "import torch\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "out = model(x)\n",
    "# print(model)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.ultis import CIFAR100read\n",
    "\n",
    "dataset = CIFAR100read(mode='train', data_path='data')\n",
    "\n",
    "print(dataset[0][0].shape, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Caltech101\n",
    "dataset = Caltech101(root='data', download=True)\n",
    "dataset[6999][0]\n",
    "# print(dataset[0][0].shape, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.ultis import Caltech101read\n",
    "from torchvision.transforms import ToPILImage\n",
    "dataset = Caltech101read(mode='train', data_path='data')\n",
    "ToPILImage()(dataset[1000][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import VOCSegmentation\n",
    "dataset = VOCSegmentation(root='data', download=False, image_set='train', transforms=None)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1449"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import VOCSegmentation\n",
    "import numpy as np\n",
    "dataset = VOCSegmentation(root='data', download=False, image_set='val', transforms=None)\n",
    "len(dataset)\n",
    "# dataset[10][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Capsule.ultis import VOC2012read\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# import albumentations as A\n",
    "import cv2\n",
    "\n",
    "# transform = A.Compose([\n",
    "#     A.RandomCrop(width=256, height=256),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.RandomBrightnessContrast(p=0.2),\n",
    "# ])\n",
    "\n",
    "dataset = VOC2012read(mode='val', data_path='data')\n",
    "ToPILImage()(dataset[30][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 10, 16, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "B = 8\n",
    "C = 10\n",
    "P = 16\n",
    "\n",
    "W_ij = nn.Conv3d(1, C * P, kernel_size=(P, 1, 1), stride=(P, 1, 1), bias=False)\n",
    "x = torch.randn(2, B, P, 4, 4)\n",
    "pre_votes = [W_ij(x_sub) for x_sub in torch.split(x, 1, dim=1)]\n",
    "out = torch.concat([pre_vote.reshape(2, 1, C, P, 4, 4) for pre_vote in pre_votes], dim=1)\n",
    "# out = out.reshape(2, C, P, 4, 4)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 torch.Size([2, 1, 16, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# torch split channel\n",
    "x = torch.randn(2, 8, 16, 4, 4)\n",
    "x = torch.split(x, 1, dim=1)\n",
    "print(len(x), x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10, 16, 4, 4]) torch.Size([8, 10, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "from Capsule.Routing import CapsuleRouting\n",
    "import torch\n",
    "routing = {\n",
    "            \"type\": \"dynamic\",\n",
    "            \"iters\": 3,\n",
    "            \"temp\": 1.5,\n",
    "        }\n",
    "routinglayer = CapsuleRouting(B=8, C=10, P=4, cap_style='c', routing=routing)\n",
    "x = torch.randn(2, 8, 16, 4, 4)\n",
    "a = torch.randn(2, 8, 4, 4)\n",
    "out, a_out = routinglayer(x, a)\n",
    "print(out.shape, a_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6040, 0.2777, 0.5767, 0.6165],\n",
      "          [0.3758, 0.6596, 0.2936, 0.4022],\n",
      "          [0.1371, 0.2124, 0.1872, 0.5220],\n",
      "          [0.6030, 0.9207, 0.6121, 0.3350]],\n",
      "\n",
      "         [[0.2275, 0.1710, 0.1875, 0.3032],\n",
      "          [0.5928, 0.1790, 0.5142, 0.3214],\n",
      "          [0.1956, 0.1329, 0.1470, 0.2669],\n",
      "          [0.3955, 0.3904, 0.2644, 0.3318]],\n",
      "\n",
      "         [[0.9401, 0.4373, 0.1585, 0.4811],\n",
      "          [0.8968, 0.3519, 0.5971, 0.4272],\n",
      "          [0.2898, 0.7556, 0.5593, 0.8210],\n",
      "          [0.4495, 0.6246, 0.9292, 0.8334]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0599, 0.1059, 0.1413, 0.3700],\n",
      "          [0.1051, 0.1277, 0.1526, 0.0729],\n",
      "          [0.0581, 0.2813, 0.3703, 0.1075],\n",
      "          [0.0943, 0.0784, 0.2443, 0.1532]],\n",
      "\n",
      "         [[0.0724, 0.5346, 0.3136, 0.1972],\n",
      "          [0.3040, 0.0528, 0.3302, 0.3278],\n",
      "          [0.6266, 0.2282, 0.1380, 0.0495],\n",
      "          [0.4101, 0.0880, 0.1419, 0.2759]],\n",
      "\n",
      "         [[0.2673, 0.0852, 0.5434, 0.4361],\n",
      "          [0.6748, 0.3486, 0.3713, 0.1408],\n",
      "          [0.0671, 0.2088, 0.2334, 0.5238],\n",
      "          [0.6784, 0.5499, 0.0723, 0.2117]]],\n",
      "\n",
      "\n",
      "        [[[0.8490, 0.1744, 0.2069, 0.1488],\n",
      "          [0.2983, 0.3388, 0.4044, 0.3163],\n",
      "          [0.1579, 0.6876, 0.5869, 0.4605],\n",
      "          [0.2522, 0.7248, 0.8854, 0.5012]],\n",
      "\n",
      "         [[0.2527, 0.2922, 0.2531, 0.5114],\n",
      "          [0.6312, 0.3162, 0.3029, 0.1144],\n",
      "          [0.2266, 0.2727, 0.3067, 0.4250],\n",
      "          [0.4256, 0.3123, 0.2772, 0.3423]],\n",
      "\n",
      "         [[0.1972, 0.5976, 0.5581, 0.4436],\n",
      "          [0.9102, 0.0802, 0.9265, 0.9544],\n",
      "          [0.9349, 0.6071, 0.1303, 0.0970],\n",
      "          [0.8745, 0.5294, 0.6302, 0.8642]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.2866, 0.1755, 0.1584, 0.0680],\n",
      "          [0.1507, 0.0856, 0.2071, 0.1695],\n",
      "          [0.1233, 0.1004, 0.1597, 0.2233],\n",
      "          [0.1683, 0.1323, 0.2308, 0.3723]],\n",
      "\n",
      "         [[0.4381, 0.2120, 0.2071, 0.0265],\n",
      "          [0.1096, 0.0947, 0.1446, 0.2256],\n",
      "          [0.1067, 0.1129, 0.2345, 0.2135],\n",
      "          [0.2091, 0.1435, 0.2525, 0.4534]],\n",
      "\n",
      "         [[0.7809, 0.4927, 0.8066, 0.5574],\n",
      "          [0.3400, 0.2726, 0.2808, 0.0472],\n",
      "          [0.3933, 0.5159, 0.0929, 0.8498],\n",
      "          [0.5033, 0.7771, 0.5606, 0.3408]]],\n",
      "\n",
      "\n",
      "        [[[0.1196, 0.8495, 0.6590, 0.0422],\n",
      "          [0.2776, 0.1438, 0.1653, 0.5423],\n",
      "          [0.4712, 0.1035, 0.1732, 0.0871],\n",
      "          [0.3993, 0.2471, 0.1489, 0.5143]],\n",
      "\n",
      "         [[0.3499, 0.4834, 0.7144, 0.4955],\n",
      "          [0.6152, 0.1224, 0.5815, 0.4711],\n",
      "          [0.8765, 0.4273, 0.1594, 0.4309],\n",
      "          [0.7821, 0.4452, 0.3819, 0.4203]],\n",
      "\n",
      "         [[0.6297, 0.4277, 0.2960, 0.3639],\n",
      "          [0.4462, 0.1298, 0.4520, 0.3822],\n",
      "          [0.3950, 0.3526, 0.1606, 0.7349],\n",
      "          [0.3484, 0.2892, 0.5093, 0.4290]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1794, 0.2230, 0.6512, 0.8934],\n",
      "          [0.6183, 0.4764, 0.6792, 0.1421],\n",
      "          [0.3659, 0.8653, 0.7952, 0.4932],\n",
      "          [0.6666, 0.8407, 0.6778, 0.3152]],\n",
      "\n",
      "         [[0.9165, 0.6815, 0.4576, 0.9459],\n",
      "          [0.9610, 0.7580, 0.6730, 0.3713],\n",
      "          [0.1385, 0.6434, 0.7464, 0.8001],\n",
      "          [0.6008, 0.9180, 0.9672, 0.7641]],\n",
      "\n",
      "         [[0.8101, 0.1662, 0.1263, 0.0703],\n",
      "          [0.3880, 0.1839, 0.6264, 0.6738],\n",
      "          [0.2287, 0.3814, 0.2736, 0.5690],\n",
      "          [0.2074, 0.5298, 0.6263, 0.6640]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.6212, 0.2698, 0.7235, 0.3718],\n",
      "          [0.8558, 0.1756, 0.6753, 0.0915],\n",
      "          [0.6509, 0.8508, 0.1603, 0.5045],\n",
      "          [0.2750, 0.9206, 0.8430, 0.6144]],\n",
      "\n",
      "         [[0.4869, 0.3607, 0.1089, 0.2902],\n",
      "          [0.1765, 0.2109, 0.1130, 0.5099],\n",
      "          [0.4408, 0.1949, 0.6065, 0.1703],\n",
      "          [0.4392, 0.4826, 0.2550, 0.4721]],\n",
      "\n",
      "         [[0.1290, 0.4031, 0.4015, 0.1562],\n",
      "          [0.4630, 0.9388, 0.6108, 0.1416],\n",
      "          [0.7801, 0.2000, 0.0604, 0.8735],\n",
      "          [0.4248, 0.3743, 0.2234, 0.8585]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.2096, 0.1895, 0.4654, 0.2846],\n",
      "          [0.1228, 0.0557, 0.4338, 0.1945],\n",
      "          [0.1296, 0.1459, 0.1317, 0.2466],\n",
      "          [0.1274, 0.0222, 0.1180, 0.2833]],\n",
      "\n",
      "         [[0.3024, 0.3413, 0.6573, 0.4500],\n",
      "          [0.1380, 0.0580, 0.5151, 0.3264],\n",
      "          [0.1192, 0.1316, 0.1991, 0.3416],\n",
      "          [0.1559, 0.0768, 0.1558, 0.3424]],\n",
      "\n",
      "         [[0.2896, 0.4434, 0.5154, 0.5777],\n",
      "          [0.1858, 0.2899, 0.3405, 0.5383],\n",
      "          [0.1728, 0.5811, 0.8652, 0.4044],\n",
      "          [0.4976, 0.1916, 0.4101, 0.1794]]],\n",
      "\n",
      "\n",
      "        [[[0.2071, 0.7389, 0.4516, 0.5862],\n",
      "          [0.1550, 0.3028, 0.4716, 0.7756],\n",
      "          [0.2086, 0.0592, 0.4400, 0.3186],\n",
      "          [0.3539, 0.4487, 0.1011, 0.3230]],\n",
      "\n",
      "         [[0.4441, 0.2644, 0.1634, 0.4825],\n",
      "          [0.5280, 0.6879, 0.2212, 0.2283],\n",
      "          [0.4120, 0.2154, 0.7191, 0.3621],\n",
      "          [0.4424, 0.3594, 0.4026, 0.7519]],\n",
      "\n",
      "         [[0.2592, 0.1580, 0.3364, 0.3607],\n",
      "          [0.2402, 0.2610, 0.4511, 0.1101],\n",
      "          [0.1409, 0.3230, 0.4517, 0.2945],\n",
      "          [0.2061, 0.0677, 0.2929, 0.3134]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.8264, 0.6815, 0.8909, 0.6033],\n",
      "          [0.6868, 0.5035, 0.3078, 0.6934],\n",
      "          [0.7206, 0.3375, 0.4633, 0.5269],\n",
      "          [0.9099, 0.5247, 0.9081, 0.6794]],\n",
      "\n",
      "         [[0.7933, 0.8343, 0.7171, 0.7420],\n",
      "          [0.8563, 0.4876, 0.5082, 0.6221],\n",
      "          [0.9645, 0.9439, 0.9117, 0.8910],\n",
      "          [0.8060, 0.9776, 0.8554, 0.7413]],\n",
      "\n",
      "         [[0.4911, 0.1469, 0.5703, 0.3067],\n",
      "          [0.5574, 0.1867, 0.7113, 0.1088],\n",
      "          [0.2757, 0.6149, 0.1641, 0.5216],\n",
      "          [0.1671, 0.3338, 0.4122, 0.5779]]],\n",
      "\n",
      "\n",
      "        [[[0.6448, 0.8768, 0.3158, 0.3499],\n",
      "          [0.6764, 0.2175, 0.1403, 0.3343],\n",
      "          [0.3806, 0.3699, 0.4399, 0.3466],\n",
      "          [0.4686, 0.9455, 0.6823, 0.3006]],\n",
      "\n",
      "         [[0.6259, 0.1637, 0.2171, 0.2033],\n",
      "          [0.4137, 0.1741, 0.0474, 0.2124],\n",
      "          [0.5194, 0.0730, 0.1496, 0.0899],\n",
      "          [0.6195, 0.4895, 0.4545, 0.4465]],\n",
      "\n",
      "         [[0.4593, 0.2233, 0.3600, 0.3258],\n",
      "          [0.2762, 0.2478, 0.1611, 0.3982],\n",
      "          [0.3433, 0.0826, 0.3454, 0.2026],\n",
      "          [0.5274, 0.2562, 0.3585, 0.3825]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.3730, 0.2762, 0.1443, 0.7050],\n",
      "          [0.1535, 0.2186, 0.1571, 0.6647],\n",
      "          [0.1030, 0.1630, 0.7072, 0.1356],\n",
      "          [0.3625, 0.1732, 0.1619, 0.2174]],\n",
      "\n",
      "         [[0.2834, 0.6341, 0.6292, 0.1619],\n",
      "          [0.4008, 0.2894, 0.6686, 0.4115],\n",
      "          [0.6012, 0.3176, 0.4022, 0.5496],\n",
      "          [0.4022, 0.0777, 0.2654, 0.7661]],\n",
      "\n",
      "         [[0.3059, 0.0838, 0.2851, 0.2691],\n",
      "          [0.1730, 0.3811, 0.1630, 0.3169],\n",
      "          [0.4416, 0.2076, 0.2596, 0.4065],\n",
      "          [0.5246, 0.0617, 0.2036, 0.4626]]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(a_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "(40000, 512, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "# inference on CIFAR10 dataset with ResNet18\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from Capsule.ultis import CIFAR10read, Caltech101read, CIFAR100read\n",
    "import numpy as np\n",
    "\n",
    "dataset = CIFAR100read(mode='train', data_path='data')\n",
    "model = models.resnet18(pretrained=True).cuda()\n",
    "model.avgpool = nn.Identity()\n",
    "model.fc = nn.Identity()\n",
    "model.eval()\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "train_list = []\n",
    "train_label = []\n",
    "\n",
    "for i, (x, y, image) in enumerate(dataloader):\n",
    "    out = model(x.cuda())\n",
    "    out = out.reshape(out.shape[0], -1, 7, 7)\n",
    "    train_list.append(out.cpu().detach().numpy())\n",
    "    train_label.append(y.numpy())\n",
    "\n",
    "train_list = np.concatenate(train_list, axis=0)\n",
    "train_label = np.concatenate(train_label, axis=0)\n",
    "print(train_list.shape)\n",
    "# save train_list and train_label to h5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('data/CIFAR100-features/cifar100_train.h5', 'w') as f:\n",
    "    f.create_dataset('data', data=train_list)\n",
    "    f.create_dataset('label', data=train_label)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6414"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import h5py\n",
    "\n",
    "hf = h5py.File('data/CIFAR100-features/cifar100_train.h5', 'r')\n",
    "train_data = np.array(hf['data'])\n",
    "train_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "hf = h5py.File('data/CIFAR100-features/cifar100_val.h5', 'r')\n",
    "test_data = np.array(hf['data'])\n",
    "test_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "\n",
    "train_data = np.mean(train_data, axis=(2, 3))\n",
    "test_data = np.mean(test_data, axis=(2, 3))\n",
    "\n",
    "#scikit-learn MLPClassifier on train_data, train_label\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(512, 512), max_iter=1000)\n",
    "clf.fit(train_data, train_label)\n",
    "accuracy_score(clf.predict(test_data), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(train_data, train_label)\n",
    "accuracy_score(clf.predict(train_data), train_label)\n",
    "accuracy_score(clf.predict(test_data), test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capsule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
