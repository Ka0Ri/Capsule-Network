{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vips/anaconda3/envs/Capsule/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/vips/anaconda3/envs/Capsule/lib/python3.9/site-packages/neptune/internal/backends/hosted_client.py:48: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
      "  from neptune.version import version as neptune_client_version\n",
      "/home/vips/anaconda3/envs/Capsule/lib/python3.9/site-packages/pytorch_lightning/loggers/neptune.py:39: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
      "  from neptune import new as neptune\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from new_Capsule.Model import *\n",
    "from new_Capsule.ultis import *\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from neptune.types import File\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architect_settings': {'name': 'capsconv', 'reconstructed': True, 'n_cls': 10, 'convs': {'channels': [1, 64, 128], 'k': 5, 's': 2, 'p': 2}, 'primay_caps': {'channels': [128, 32], 'k': 3, 's': 2, 'p': 1}, 'caps': {'init': 'noisy_identity', 'cap_dims': 4, 'channels': [32, 16, 10], 'k': 3, 's': 1, 'p': 0, 'routing': {'type': 'fuzzy', 'params': [3]}}}, 'training_settings': {'loss': 'spread', 'CAM': False, 'lr': 0.0001, 'lr_step': 10, 'lr_decay': 0.8, 'momentum': 0.9, 'weight_decay': 0.005, 'n_epoch': 100, 'n_batch': 128, 'dataset': 'center-Mnist', 'ckpt_path': 'test'}}\n"
     ]
    }
   ],
   "source": [
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "with open(\"new_Capsule/config.yaml\", 'r') as stream:\n",
    "    PARAMS = yaml.safe_load(stream)\n",
    "    PARAMS = PARAMS['config2']\n",
    "    print(PARAMS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class CapsuleModel(LightningModule):\n",
    "    def __init__(self, PARAMS):\n",
    "        super().__init__()\n",
    "\n",
    "        self.reconstructed = False\n",
    "        self.segment = False\n",
    "       \n",
    "        self.architect_settings = PARAMS[\"architect_settings\"]\n",
    "        self.train_settings = PARAMS[\"training_settings\"]\n",
    "        assert self.architect_settings['name'] in [\"conv\", \"Unet\", \"capsconv\", \"capsUnet\", \"eff-capsconv\", \"eff-capsUnet\"], \\\n",
    "                    \"Model is not implemented yet\"\n",
    "        \n",
    "        # Model selection\n",
    "        if(self.architect_settings['name'] == \"conv\"):\n",
    "            self.model = ConvNeuralNet(model_configs=self.architect_settings)\n",
    "        elif(self.architect_settings['name'] in [\"capsconv\", \"eff-capsconv\"]):\n",
    "            self.model = CapNets(model_configs=self.architect_settings)\n",
    "        elif(self.architect_settings['name'] in [\"capsUnet\", \"eff-capsUnet\"]):\n",
    "            self.segment = True\n",
    "            self.model = CapConvUNet(model_configs=self.architect_settings)\n",
    "        elif(self.architect_settings['name'] == \"Unet\"):\n",
    "            self.segment = True\n",
    "            self.model = ConvUNet(model_configs=self.architect_settings)\n",
    "\n",
    "        assert self.train_settings['loss']  in [\"ce\", \"spread\", \"bce\", \"mse\", \"margin\", \"dice\"], \\\n",
    "                                                \"Loss is not implemented yet\"\n",
    "        # Loss selection\n",
    "        if(self.train_settings['loss'] == \"ce\"):                                                                                                                \n",
    "            self.loss = CrossEntropyLoss(num_classes=self.architect_settings['n_cls'])\n",
    "        elif(self.train_settings['loss'] == \"spread\"):\n",
    "            self.loss = SpreadLoss(num_classes=self.architect_settings['n_cls'])\n",
    "        elif(self.train_settings['loss'] == \"margin\"):\n",
    "            self.loss = MarginLoss(num_classes=self.architect_settings['n_cls'])\n",
    "        elif(self.train_settings['loss'] == \"bce\"):\n",
    "            self.loss = BCE()\n",
    "        elif(self.train_settings['loss'] == \"mse\"):\n",
    "            self.loss = MSE()\n",
    "        elif(self.train_settings['loss'] == \"dice\"):\n",
    "            self.loss = DiceLoss()\n",
    "      \n",
    "        # For reconstruction purposes (regularizer)\n",
    "        if(self.architect_settings['reconstructed']):\n",
    "            self.reconstructed = True\n",
    "            self.reconstruct_loss = MSE()\n",
    "\n",
    "        # For visualizaiton purposes\n",
    "        if(self.train_settings['CAM']):\n",
    "            self.features_blobs = []\n",
    "            # for CAM visualization\n",
    "            if(\"caps\" in self.architect_settings['name']):\n",
    "                def hook_feature(module, input, output):\n",
    "                    self.features_blobs.append(np.array(output[0].tolist()))\n",
    "                self.model.caps_layers[0].register_forward_hook(hook_feature)\n",
    "                self.model.caps_layers[1].register_forward_hook(hook_feature)\n",
    "            else:\n",
    "                def hook_feature(module, input, output):\n",
    "                    self.features_blobs.append(np.array(output.tolist()))\n",
    "                self.model.caps_layers[0].register_forward_hook(hook_feature)\n",
    "\n",
    "                \n",
    "        # For logging purposes\n",
    "        # self.log(\"metrics/no_parameters\", count_parameters(self.model))\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        return self.model(x, y)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        if(self.segment):\n",
    "            y_hat = self(x)\n",
    "            loss = self.loss(y_hat, y)\n",
    "            y_true = y.cpu().detach()\n",
    "            y_pred = y_hat.cpu().detach()\n",
    "            \n",
    "        else:\n",
    "            if(self.reconstructed):\n",
    "                y_hat, reconstructions = self(x, y)\n",
    "                loss = self.loss(y_hat, y) + 0.1 * self.reconstruct_loss(reconstructions, x)\n",
    "            else:\n",
    "                y_hat = self(x)\n",
    "                loss = self.loss(y_hat, y)\n",
    "            y_true = y.tolist()\n",
    "            y_pred = y_hat.argmax(axis=1).tolist()\n",
    "            self.log(\"metrics/batch/acc\", accuracy_score(y_true, y_pred), prog_bar=False)\n",
    "\n",
    "        self.log(\"metrics/batch/loss\", loss, prog_bar=False)\n",
    "        self.training_step_outputs.append({\"loss\": loss.item(), \"y_true\": y_true, \"y_pred\": y_pred})\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        outputs = self.training_step_outputs\n",
    "        loss = np.array([])\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "        for results_dict in outputs:\n",
    "            loss = np.append(loss, results_dict[\"loss\"])\n",
    "            y_true = np.append(y_true, results_dict[\"y_true\"])\n",
    "            y_pred = np.append(y_pred, results_dict[\"y_pred\"])\n",
    "       \n",
    "        self.log(\"metrics/epoch/loss\", loss.mean())\n",
    "        if(self.segment == False):\n",
    "            self.log(\"metrics/epoch/acc\", accuracy_score(y_true, y_pred))\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "       \n",
    "        if(self.segment):\n",
    "            y_hat = self(x)\n",
    "            loss = self.loss(y_hat, y)\n",
    "            y_true = y.cpu().detach()\n",
    "            y_pred = torch.sigmoid(y_hat).cpu().detach()\n",
    "            self.validation_step_outputs.append({\"loss\": loss.item(), \"y_true\": y_true, \"y_pred\": y_pred})\n",
    "\n",
    "        else:\n",
    "            if(self.reconstructed):\n",
    "                y_hat, reconstructions = self(x, y)\n",
    "                loss = self.loss(y_hat, y) + 0.1 * self.reconstruct_loss(reconstructions, x)\n",
    "                y_true = y.tolist()\n",
    "                y_pred = y_hat.argmax(axis=1).tolist()\n",
    "                self.validation_step_outputs.append({\"loss\": loss.item(), \"y_true\": y_true, \"y_pred\": y_pred, \"reconstructions\": reconstructions})\n",
    "\n",
    "            else:\n",
    "                y_hat = self(x)\n",
    "                loss = self.loss(y_hat, y)\n",
    "                y_true = y.tolist()\n",
    "                y_pred = y_hat.argmax(axis=1).tolist()\n",
    "                self.validation_step_outputs.append({\"loss\": loss.item(), \"y_true\": y_true, \"y_pred\": y_pred})\n",
    "    \n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.validation_step_outputs\n",
    "        loss = np.array([])\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "        for results_dict in outputs:\n",
    "            loss = np.append(loss, results_dict[\"loss\"])\n",
    "            y_true = np.append(y_true, results_dict[\"y_true\"])\n",
    "            y_pred = np.append(y_pred, results_dict[\"y_pred\"])\n",
    "        self.log(\"val/epoch/loss\", loss.mean())\n",
    "        if(self.segment):\n",
    "            y_true = make_grid(outputs[0][\"y_true\"], nrow=int(self.train_settings[\"n_batch\"] ** 0.5))\n",
    "            y_pred = make_grid(outputs[0][\"y_pred\"], nrow=int(self.train_settings[\"n_batch\"] ** 0.5))\n",
    "            y_true = y_true.numpy().transpose(1, 2, 0)\n",
    "            y_pred = y_pred.numpy().transpose(1, 2, 0)\n",
    "            \n",
    "            self.logger.experiment[\"val/gt_images\"].append(File.as_image(y_true))\n",
    "            self.logger.experiment[\"val/outputs\"].append(File.as_image(y_pred))\n",
    "        else:\n",
    "            self.log(\"val/epoch/acc\", accuracy_score(y_true, y_pred))\n",
    "            if(self.reconstructed):\n",
    "                reconstructions = make_grid(outputs[0][\"reconstructions\"], nrow=int(self.train_settings[\"n_batch\"] ** 0.5))\n",
    "                reconstructions = reconstructions.cpu().numpy().transpose(1, 2, 0)\n",
    "                self.logger.experiment[\"val/reconstructions\"].append(File.as_image(reconstructions))\n",
    "\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.train_settings['lr'])\n",
    "        # optimizer = torch.optim.SGD(self.parameters(), lr=self.train_settings['lr'], \n",
    "        #                             weight_decay=self.train_settings['weight_decay'])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=self.train_settings['lr_step'], \n",
    "                                                    gamma=self.train_settings['lr_decay'])\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### affine Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "Train_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            # transforms.RandomAffine(degrees=0, translate=(0.125, 0.125)),\n",
    "            transforms.ToTensor(),# default : range [0, 255] -> [0.0,1.0]\n",
    "            # transforms.Normalize(mean = (0.5,), std = (0.5,))\n",
    "        ])\n",
    "    \n",
    "Train_data = affNistread(mode=\"train\", data_path=\"data/affMnist\", transform=Train_transform)\n",
    "Val_data =  affNistread(mode=\"val\", data_path=\"data/affMnist\", transform=Train_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### center Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "Train_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            # transforms.RandomAffine(degrees=0, translate=(0.125, 0.125)),\n",
    "            transforms.ToTensor(),# default : range [0, 255] -> [0.0,1.0]\n",
    "            transforms.Normalize(mean = (0.5,), std = (0.5,))\n",
    "        ])\n",
    "Test_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomAffine(degrees=30, translate=(0.2, 0.2)),\n",
    "            transforms.ToTensor(),# default : range [0, 255] -> [0.0,1.0]\n",
    "            transforms.Normalize(mean = (0.5,), std = (0.5,))\n",
    "        ])\n",
    "\n",
    "Train_data = affNistread(mode=\"train\", data_path=\"data/centerMnist\", aff=False, transform=Train_transform)\n",
    "Val_data =  affNistread(mode=\"val\", data_path=\"data/centerMnist\", aff=False, transform=Train_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### smallNorb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "Train_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32),\n",
    "            transforms.ColorJitter(brightness=0.5),\n",
    "            transforms.ToTensor(),# default : range [0, 255] -> [0.0,1.0]\n",
    "            # transforms.Normalize(mean = (0.5,), std = (0.5,))\n",
    "            ])\n",
    "            \n",
    "Test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),# default : range [0, 255] -> [0.0,1.0]\n",
    "    # transforms.Normalize(mean = (0.5,), std = (0.5,))\n",
    "])\n",
    "\n",
    "Train_data = SmallNorbread(mode=\"train\", data_path=\"data/smallNorb\", transform=Train_transform)\n",
    "Val_data = SmallNorbread(mode=\"val\", data_path=\"data/smallNorb\", transform=Test_transform)\n",
    "Test_data = SmallNorbread(mode=\"test\", data_path=\"data/smallNorb\", transform=Test_transform)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lung CT Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "Train_transform = transforms.Compose([\n",
    "            # transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),# default : range [0, 255] -> [0.0,1.0]\n",
    "            # transforms.Normalize((0.5), (0.5))\n",
    "        ])\n",
    "\n",
    "Test_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomAffine(degrees=30, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),# default : range [0, 255] -> [0.0,1.0]\n",
    "            # transforms.Normalize((0.5), (0.5))\n",
    "        ])\n",
    "\n",
    "train_ds = LungCTscan(data_dir=\"data/CT-scan-dataset\", transform=Train_transform)\n",
    "val_ds = LungCTscan(data_dir=\"data/CT-scan-dataset\", transform=Train_transform)\n",
    "Train_data, _ = random_split(train_ds, [200, 67])\n",
    "_, Val_data = random_split(val_ds, [200, 67])\n",
    "_, Test_data = random_split(val_ds, [200, 67])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vips/anaconda3/envs/Capsule/lib/python3.9/site-packages/pytorch_lightning/loggers/neptune.py:358: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  self._run_instance = neptune.init_run(**self._neptune_init_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/kaori/Capsule/e/CAP-272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vips/anaconda3/envs/Capsule/lib/python3.9/site-packages/pytorch_lightning/loggers/neptune.py:406: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'list'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "  self.run[parameters_key] = params\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "neptune_logger = NeptuneLogger(\n",
    "        project=\"kaori/Capsule\",\n",
    "        api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyZjZiMDA2YS02MDM3LTQxZjQtOTE4YS1jODZkMTJjNGJlMDYifQ==\",\n",
    "        log_model_checkpoints=False,\n",
    "        # tags=[\"full-config\"]\n",
    ")\n",
    "\n",
    "neptune_logger.log_hyperparams(params=PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "Train_dataloader = DataLoader(dataset=Train_data, batch_size = PARAMS['training_settings']['n_batch'], shuffle=True, num_workers=4)\n",
    "Val_dataloader = DataLoader(dataset=Val_data, batch_size = PARAMS['training_settings']['n_batch'], shuffle=False, num_workers=4)\n",
    "# Test_dataloader = DataLoader(dataset=Test_data, batch_size = PARAMS['training_settings']['n_batch'], shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vips/anaconda3/envs/Capsule/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "model = CapsuleModel(PARAMS=PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/vips/anaconda3/envs/Capsule/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/vips/share/Vu/Capsule-Network/model/test exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | model            | CapNets    | 2.3 M \n",
      "1 | loss             | SpreadLoss | 0     \n",
      "2 | reconstruct_loss | MSE        | 0     \n",
      "------------------------------------------------\n",
      "2.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 M     Total params\n",
      "9.369     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 391/391 [00:30<00:00, 13.02it/s, v_num=-272]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 391/391 [00:30<00:00, 13.02it/s, v_num=-272]\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=os.path.join(\"model\", PARAMS['training_settings'][\"ckpt_path\"]), \n",
    "                                    save_top_k=1, monitor='metrics/batch/loss', mode=\"min\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=neptune_logger,\n",
    "    max_epochs=PARAMS['training_settings'][\"n_epoch\"],\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[1],\n",
    "    callbacks=[checkpoint_callback]\n",
    "    )\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=Train_dataloader, val_dataloaders=Val_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    img = img - np.min(img)\n",
    "    nor_img = img / np.max(img)\n",
    "    nor_img = np.uint8(255 * nor_img)\n",
    "    return nor_img\n",
    "\n",
    "def returnCAM(img, feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (40, 40)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "   \n",
    "    weight_softmax = weight_softmax.sum(axis=(2, 3))\n",
    "    feature_conv = feature_conv[0]\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = normalize(cam)\n",
    "       \n",
    "        output_cam.append(cv2.resize(cam, size_upsample))\n",
    "\n",
    "    img = cv2.cvtColor(np.array(img),cv2.COLOR_GRAY2RGB)\n",
    "    height, width, _ = img.shape\n",
    "    CAM = cv2.resize(output_cam[0], (width, height))\n",
    "    heatmap = cv2.applyColorMap(CAM, cv2.COLORMAP_JET)\n",
    "    result = heatmap * 0.3 + img * 0.5\n",
    "    \n",
    "    return normalize(result)\n",
    "\n",
    "def CapsuleAttentionMap(img, feature_conv, global_feature, class_idx):\n",
    "\n",
    "    size_upsample = (40, 40)\n",
    "    bz, nc, p, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    \n",
    "    global_feature = global_feature[0].squeeze()\n",
    "    feature_conv = feature_conv[0].squeeze().transpose(2, 3, 0, 1)\n",
    "    # feature_conv = feature_conv\n",
    "    \n",
    "    for idx in class_idx:\n",
    "        cam = feature_conv.dot(global_feature[idx])\n",
    "        cam = cam.sum(axis=-1)\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = normalize(cam)\n",
    "       \n",
    "        output_cam.append(cv2.resize(cam, size_upsample))\n",
    "\n",
    "    img = cv2.cvtColor(np.array(img),cv2.COLOR_GRAY2RGB)\n",
    "    height, width, _ = img.shape\n",
    "    CAM = cv2.resize(output_cam[0], (width, height))\n",
    "    heatmap = cv2.applyColorMap(CAM, cv2.COLORMAP_JET)\n",
    "    result = heatmap * 0.3 + img * 0.5\n",
    "    \n",
    "    return normalize(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vips/anaconda3/envs/Capsule/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "with open(\"new_Capsule/config.yaml\", 'r') as stream:\n",
    "    try:\n",
    "        PARAMS = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "conv_params = PARAMS[\"config2\"]\n",
    "conv_params[\"architect_settings\"][\"name\"] = \"conv\"\n",
    "conv_params[\"training_settings\"][\"CAM\"] = True\n",
    "conv_params[\"architect_settings\"][\"reconstructed\"] = False\n",
    "convmodel = CapsuleModel.load_from_checkpoint(\"model/demo/conv.ckpt\", PARAMS=conv_params)\n",
    "convmodel.eval()\n",
    "\n",
    "caps_params = PARAMS[\"config2\"]\n",
    "caps_params[\"architect_settings\"][\"name\"] = \"capsconv\"\n",
    "caps_params[\"architect_settings\"][\"reconstructed\"] = True\n",
    "caps_params[\"training_settings\"][\"CAM\"] = True\n",
    "capsule = CapsuleModel.load_from_checkpoint(\"model/demo/fuzzy-caps.ckpt\", PARAMS=caps_params)\n",
    "capsule.eval()\n",
    "\n",
    "Test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),# default : range [0, 255] -> [0.0,1.0]\n",
    "            transforms.Normalize(mean = (0.5,), std = (0.5,))\n",
    "        ])\n",
    "\n",
    "Test_data = affNistread(mode=\"test\", data_path=\"data/affMnist\", transform=Test_transform, aff=True)\n",
    "\n",
    "def load_data():\n",
    "    index = random.randint(0, 1000)\n",
    "    x, y = Test_data[index]\n",
    "    img = transforms.ToPILImage()(x)\n",
    "    dummy = torch.zeros_like(x)\n",
    "    x_batch = torch.stack([x, dummy])\n",
    "\n",
    "    # Convolution\n",
    "    cls = convmodel(x_batch)[0]\n",
    "    cls = torch.softmax(cls, dim=-1).tolist()\n",
    "    labels = {k: float(v) for k, v in enumerate(cls)}\n",
    "\n",
    "    #visualize CAM\n",
    "    params = list(convmodel.parameters())\n",
    "    weight_softmax = np.squeeze(params[-4].tolist())\n",
    "    features_map = np.array(convmodel.features_blobs[-1])\n",
    "   \n",
    "    activation_map = returnCAM(img, features_map, weight_softmax, class_idx=[np.argmax(cls)])\n",
    "    \n",
    "    # Capsule\n",
    "    cls2, _ = capsule(x_batch)\n",
    "    cls2 = cls2[0].tolist()\n",
    "    # cls2 = torch.softmax(cls2[0], dim=-1).tolist()\n",
    "    labels2 = {k: float(v) for k, v in enumerate(cls2)}\n",
    "\n",
    "    features_map = np.array(capsule.features_blobs[-2])\n",
    "    global_map = np.array(capsule.features_blobs[-1])\n",
    "\n",
    "    activation_map2 = CapsuleAttentionMap(img, features_map, global_map, class_idx=[np.argmax(cls2)])\n",
    "\n",
    "    return img, \"class {}\".format(y), labels, labels2, activation_map, activation_map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import gradio as gr\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def run():\n",
    "    import time\n",
    "    for i in range(0, 10):\n",
    "        time.sleep(1)\n",
    "        yield load_data()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    gr.Markdown(\"## Image Examples\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            im = gr.Image().style(width=200, height=200)\n",
    "            conv_explain = gr.Image(label= \"Activation map\").style(width=300, height=300)\n",
    "            capsule_explain = gr.Image(label=\"Capsule Activation map\").style(width=300, height=300)\n",
    "        with gr.Column():\n",
    "            cls_box = gr.Textbox(label=\"True Image class\")\n",
    "            label_conv = gr.Label(label=\"Convolutional Model\", num_top_classes=4)\n",
    "            label_capsule = gr.Label(label=\"Capsule model\", num_top_classes=4)\n",
    "            btn = gr.Button(value=\"Load random image\")\n",
    "    btn.click(run, inputs=None, outputs=[im, cls_box, label_conv, label_capsule, conv_explain, capsule_explain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://a214d5282fd26301ea.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a214d5282fd26301ea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "demo.queue()\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capsule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
