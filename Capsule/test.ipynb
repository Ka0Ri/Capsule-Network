{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import CapsuleRouting\n",
    "import torch\n",
    "routing = CapsuleRouting(mode='em')\n",
    "\n",
    "v = torch.rand(2, 8, 10, 16, 1, 1)\n",
    "a = torch.rand(2, 8, 1, 1)\n",
    "v, a = routing(v, a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import AdaptiveCapsuleHead\n",
    "import torch\n",
    "\n",
    "model = AdaptiveCapsuleHead(32, 10, 4, 1, False, *[\"dynamic\", 10, 1.5])\n",
    "v = torch.rand(2, 32, 5, 5)\n",
    "\n",
    "out, v = model(v, get_capsules=True)\n",
    "print(v.shape)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from Capsule.Layer.Classifier import CapsuleWrappingClassifier\n",
    "with open('config/new-config.yml', 'r') as stream:\n",
    "    PARAMS = yaml.safe_load(stream)\n",
    "    print(PARAMS)\n",
    "model = CapsuleWrappingClassifier(model_configs=PARAMS['architect_settings'])\n",
    "\n",
    "img = torch.rand(2, 3, 224, 224)\n",
    "\n",
    "out = model(img)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.ultis import Cub2011\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "dataset = Cub2011(mode='test', data_path='data/CUB_200_2011', transform=None)\n",
    "\n",
    "img, label, oimg = dataset[10]\n",
    "print(label)\n",
    "plt.imshow(ToPILImage()(oimg))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.arange(60.).reshape(3,4,5)\n",
    "b = np.arange(24.).reshape(4,3,2)\n",
    "print(a)\n",
    "print(b)\n",
    "np.einsum('ijk,jil->kl', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "A = torch.rand(2, 15, 4, 4, 3, 3)\n",
    "B = torch.rand(15, 4, 4, 8)\n",
    "v = torch.einsum('bBijHW, BjkC -> bBCikHW', A, B)\n",
    "print(v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS =  {\n",
    "    \"architect_settings\" : {\n",
    "            \"task\": \"None\",\n",
    "            \"name\": \"model-test\",\n",
    "            \"backbone\": {\n",
    "                    \"name\": \"resnet-s\",\n",
    "                    \"is_full\": False,\n",
    "                    \"is_pretrained\": True,\n",
    "                    \"is_feats\": False,\n",
    "                    \"is_freeze\": True, \n",
    "            },\n",
    "            \"n_cls\": 2,\n",
    "            \"is_caps\": False,\n",
    "            \"caps\":{\n",
    "                \"mode\": 1,\n",
    "                \"cap_dims\": 4,\n",
    "                \"routing\":{\n",
    "                        \"type\": \"dynamic\",\n",
    "                        \"params\": [3, 0.01, 1.5]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "    \"dataset_settings\": {\n",
    "            \n",
    "            },\n",
    "    \"training_settings\":{\n",
    "    \n",
    "    }\n",
    "}\n",
    "\n",
    "from Capsule.Layer.Classifier import CapsuleWrappingClassifier\n",
    "from Capsule.ultis import Cub2011, CIFAR10read\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "model = CapsuleWrappingClassifier(model_configs=PARAMS[\"architect_settings\"])\n",
    "model.eval()\n",
    "\n",
    "def get_train_data(model, dataloder):\n",
    "    train_data = []\n",
    "    train_label = []    \n",
    "    for batch in dataloder:\n",
    "        img, label, oimg = batch\n",
    "        feats = model.backbone(img)\n",
    "        out = feats.reshape(-1, 512, 7, 7)\n",
    "        train_data.append(out.detach().numpy())\n",
    "        train_label.append(label.detach().numpy())\n",
    "\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    train_label = np.concatenate(train_label, axis=0)\n",
    "    print(train_data.shape)\n",
    "    print(train_label.shape)\n",
    "    return train_data, train_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10read(mode=\"train\", data_path=\"data\")\n",
    "dataloder = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "train_data, train_label = get_train_data(model, dataloder)\n",
    "\n",
    "hf = h5py.File('data/CIFAR10_train_data.h5', 'w')\n",
    "hf.create_dataset('data', data=train_data)\n",
    "hf.create_dataset('label', data=train_label)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10read(mode=\"test\", data_path=\"data\")\n",
    "dataloder = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "test_data, test_label = get_train_data(model, dataloder)\n",
    "\n",
    "hf = h5py.File('data/CIFAR10_test_data.h5', 'w')\n",
    "hf.create_dataset('data', data=test_data)\n",
    "hf.create_dataset('label', data=test_label)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import h5py\n",
    "\n",
    "hf = h5py.File('data/CIFAR10_train_data.h5', 'r')\n",
    "train_data = np.array(hf['data'])\n",
    "train_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "hf = h5py.File('data/CIFAR10_test_data.h5', 'r')\n",
    "test_data = np.array(hf['data'])\n",
    "test_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "\n",
    "train_data = np.mean(train_data, axis=(2, 3))\n",
    "test_data = np.mean(test_data, axis=(2, 3))\n",
    "\n",
    "#scikit-learn MLPClassifier on train_data, train_label\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(512, 512), max_iter=1000)\n",
    "clf.fit(train_data, train_label)\n",
    "accuracy_score(clf.predict(test_data), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(train_data, train_label)\n",
    "accuracy_score(clf.predict(train_data), train_label)\n",
    "accuracy_score(clf.predict(test_data), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hf = h5py.File('data/CUB_ResNet_train_data.h5', 'r')\n",
    "train_data = np.array(hf['data'])\n",
    "train_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "\n",
    "tensor_x = torch.Tensor(train_data) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(train_label)\n",
    "my_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "train_loader = DataLoader(my_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('data/CUB_ResNet_test_data.h5', 'r')\n",
    "test_data = np.array(hf['data'])\n",
    "test_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "\n",
    "tensor_x = torch.Tensor(test_data) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(test_label)\n",
    "my_val_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "valid_loader = DataLoader(my_val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from Capsule.CapsuleLayer import AdaptiveCapsuleHead\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "                AdaptiveCapsuleHead(512, 200,\n",
    "                4, 4, True, \n",
    "                *['dynamic', 3, 2]),\n",
    "                # nn.LogSoftmax(dim=1),\n",
    "                nn.Flatten(start_dim=1)\n",
    "            )\n",
    "\n",
    "# Define the classifier on top of the ResNet\n",
    "# classifier = nn.Sequential(\n",
    "#     nn.Linear(512, 512, 1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(512, 512, 1),\n",
    "#     # nn.LogSoftmax(dim=1),\n",
    "#     # nn.Flatten(start_dim=1)\n",
    "# )\n",
    "\n",
    "# # Combine the ResNet and classifier\n",
    "# from torchvision.models import resnet18\n",
    "# resnet = resnet18(pretrained=True)\n",
    "# resnet.fc = nn.Identity()\n",
    "# model = nn.Sequential(resnet, classifier)\n",
    "model = classifier.to(device)\n",
    "# Load the MNIST dataset\n",
    "# train_dataset = Cub2011(mode=\"train\", data_path=\"data/CUB_200_2011\")\n",
    "# valid_dataset = Cub2011(mode=\"val\", data_path=\"data/CUB_200_2011\")\n",
    "\n",
    "# # Set hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "# # Create data loaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "max_val = 0\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/ {num_epochs} - Training\"):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        # print(images, labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Compute average training accuracy and loss\n",
    "    train_accuracy = 100.0 * train_correct / total_train_samples\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_correct = 0\n",
    "    total_valid_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(valid_loader, desc=f\"Epoch {epoch + 1}/ {num_epochs} - Validation\"):\n",
    "            # Forward pass\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            valid_correct += (predicted == labels).sum().item()\n",
    "            total_valid_samples += labels.size(0)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    # Compute average validation accuracy and loss\n",
    "    valid_accuracy = 100.0 * valid_correct / total_valid_samples\n",
    "    valid_loss /= len(valid_loader)\n",
    "    if(valid_accuracy > max_val):\n",
    "        max_val = valid_accuracy\n",
    "    # Print epoch-wise results\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Valid Loss: {valid_loss:.4f} | Valid Accuracy: {valid_accuracy:.2f}%\")\n",
    "print(f\"Max Valid Accuracy: {max_val:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUB\n",
    "- CUB-resnet-sci-SVM: 0.5904\n",
    "- CUB-resnet-freeze-2hMLP: 0.5856\n",
    "- CUB-resnet-sci-MLP: 0.577\n",
    "\n",
    "CIFAR\n",
    "- CIFAR-resnet-sci-MLP: 0.8717\n",
    "- CIFAR-resnet-sci-SVM: 0.8822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import CapsuleRouting\n",
    "import torch\n",
    "p = torch.randn(2, 32, 16, 4, 4)\n",
    "a = torch.randn(2, 32, 4, 4)\n",
    "params = {\"reduce\": True,\n",
    "        \"cap_dims\": 4,\n",
    "        \"routing\":{\n",
    "                \"type\": \"em\",\n",
    "                \"iters\": 3,\n",
    "                \"temp\": 1.5,\n",
    "        }\n",
    "}\n",
    "\n",
    "dynamic_routing = CapsuleRouting(B=32, C=10, caps=params)\n",
    "dynamic_routing.eval()\n",
    "v_out, a_out = dynamic_routing(p, a)\n",
    "print(v_out.shape, a_out.shape)\n",
    "print(a_out, v_out)\n",
    "# print(dynamic_routing.W_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import AdaptiveCapsuleHead\n",
    "import torch\n",
    "p = torch.randn(2, 512, 7, 7)\n",
    "# a = torch.randn(2, 512, 7, 7)\n",
    "params = {\n",
    "    \"n_cls\": 10,\n",
    "    \"n_layers\": 2,\n",
    "    \"n_emb\": 512,\n",
    "    \"is_caps\": False,\n",
    "    \"caps\": {\n",
    "        \"pooling\": \"None\",\n",
    "        \"shuffle\": True,\n",
    "        \"cap_style\": \"c\", # c: by channel, hw: by height and width\n",
    "        \"cap_dims\": 4,\n",
    "        \"routing\":{\n",
    "            \"reduce\": True,\n",
    "            \"type\": \"dynamic\",\n",
    "            \"iters\": 3,\n",
    "            \"temp\": 1.5,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "adaptive_capsule_head = AdaptiveCapsuleHead(512, head=params)\n",
    "adaptive_capsule_head.eval()\n",
    "a_out = adaptive_capsule_head(p)\n",
    "print(a_out.shape)\n",
    "print(a_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.CapsuleLayer import ProjectionHead\n",
    "import torch\n",
    "p = torch.randn(2, 512, 7, 7)\n",
    "\n",
    "projection_head = ProjectionHead(512, 32, 1)\n",
    "print(projection_head)\n",
    "out = projection_head(p)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptivePoolCapsuleHead(nn.Module):\n",
    "    '''\n",
    "    Capsule Header combines of Primary Capsule and Linear Capsule.\n",
    "    \n",
    "    - Arguments:\n",
    "        B: number of capsule in L layer\n",
    "        C: number of capsule in L + 1 layer\n",
    "        get_capsules: Return Capsules's vector\n",
    "        P: size of a capsule\n",
    "        reduce: reduce the featrue maps before routing\n",
    "        args: arguments for routing method\n",
    "        argv[0]: routing method\n",
    "        argv[1]: number of iteration\n",
    "        argv[2]: m for fuzzy routing\n",
    "    '''\n",
    "    def __init__(self, B, head):\n",
    "        super(AdaptivePoolCapsuleHead, self).__init__()\n",
    "     \n",
    "  \n",
    "        self.reduce = head['caps']['reduce']\n",
    "        self.n_layers = head['n_layers']\n",
    "        n_emb = head['n_emb']\n",
    "        self.P = head['caps']['cap_dims']\n",
    "\n",
    "        assert B % (self.P * self.P) == 0, \"channel is not divisible by P * P\"\n",
    "        self.B = B // (self.P * self.P)\n",
    "        assert n_emb % (self.P * self.P) == 0, \"embedding is not divisible by P * P\"\n",
    "        self.n_emb = n_emb // (self.P * self.P)\n",
    "      \n",
    "        self.primary_capsule = nn.Sequential()\n",
    "        self.a_routing = nn.Sequential()\n",
    "        if(self.reduce):\n",
    "            self.primary_capsule.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "            self.a_routing.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        if(self.n_layers == 1):\n",
    "            self.primary_capsule.append(nn.Identity())\n",
    "            self.routinglayer = CapsuleRouting(self.B, head['n_cls'], head['caps'])\n",
    "        else:\n",
    "            self.primary_capsule.append(nn.Conv2d(B, n_emb, 1))\n",
    "            self.primary_capsule.append(nn.ReLU())\n",
    "            for i in range(1, self.n_layers - 1):\n",
    "                self.primary_capsule.append(nn.Conv2d(n_emb, n_emb, 1))\n",
    "                if(i < self.n_layers - 1):\n",
    "                    self.primary_capsule.append(nn.ReLU())\n",
    "            self.routinglayer = CapsuleRouting(self.n_emb, head['n_cls'], head['caps'])\n",
    "\n",
    "        self.a_routing.append(nn.Conv2d(B, self.B, 1))\n",
    "\n",
    "    def forward(self, x, get_capsules=False):\n",
    "        '''\n",
    "        input: \n",
    "            tensor 4D (b, B, h, w)\n",
    "        output:\n",
    "            capsule 3D (b, C, P*P) / 5D (b, C, P*P, h, w)\n",
    "            activation 2D (b, C) / 5D (b, C, h, w)\n",
    "        '''\n",
    "        # Primary capsule\n",
    "        \n",
    "        # p <- (b, B, P * P, h, w)\n",
    "        # a <- (b, B, h, w)\n",
    "        p = self.primary_capsule(x)\n",
    "        # x <- (b, C, h, w)\n",
    "        b, d, h, w =  p.shape\n",
    "        p = p.reshape(b, self.B , self.P ** 2, h, w)\n",
    "       \n",
    "        a = self.a_routing(x)\n",
    "        a = torch.sigmoid(a)\n",
    "        \n",
    "        print(p.shape, a.shape)\n",
    "        p_out, a_out = self.routinglayer(p, a)\n",
    "        a_out = torch.log(a_out / (1 - a_out + EPS))\n",
    "       \n",
    "        if get_capsules:\n",
    "            return p_out, a_out\n",
    "        else: \n",
    "            return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "orthogonal_weight = torch.nn.utils.parametrizations.orthogonal(nn.Conv2d(2, 2, 3))\n",
    "print(orthogonal_weight.weight[0][0].T @ orthogonal_weight.weight[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.model import CapsuleWrappingClassifier, CapsuleWrappingSegment\n",
    "import yaml\n",
    "with open('config/new-config.yml', 'r') as stream:\n",
    "        PARAMS = yaml.safe_load(stream)\n",
    "        print(PARAMS)\n",
    "\n",
    "model = CapsuleWrappingSegment(PARAMS['architect_settings'])\n",
    "\n",
    "import torch\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "out = model(x)\n",
    "# print(model)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.ultis import CIFAR100read\n",
    "\n",
    "dataset = CIFAR100read(mode='train', data_path='data')\n",
    "\n",
    "print(dataset[0][0].shape, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Caltech101\n",
    "dataset = Caltech101(root='data', download=True)\n",
    "dataset[6999][0]\n",
    "# print(dataset[0][0].shape, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.ultis import Caltech101read\n",
    "from torchvision.transforms import ToPILImage\n",
    "dataset = Caltech101read(mode='train', data_path='data')\n",
    "ToPILImage()(dataset[1000][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VOCSegmentation\n",
    "dataset = VOCSegmentation(root='data', download=False, image_set='train', transforms=None)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VOCSegmentation\n",
    "import numpy as np\n",
    "dataset = VOCSegmentation(root='data', download=False, image_set='val', transforms=None)\n",
    "len(dataset)\n",
    "# dataset[10][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.ultis import VOC2012read\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# import albumentations as A\n",
    "import cv2\n",
    "\n",
    "# transform = A.Compose([\n",
    "#     A.RandomCrop(width=256, height=256),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.RandomBrightnessContrast(p=0.2),\n",
    "# ])\n",
    "\n",
    "dataset = VOC2012read(mode='val', data_path='data')\n",
    "ToPILImage()(dataset[30][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "B = 8\n",
    "C = 10\n",
    "P = 16\n",
    "\n",
    "W_ij = nn.Conv3d(1, C * P, kernel_size=(P, 1, 1), stride=(P, 1, 1), bias=False)\n",
    "x = torch.randn(2, B, P, 4, 4)\n",
    "pre_votes = [W_ij(x_sub) for x_sub in torch.split(x, 1, dim=1)]\n",
    "out = torch.concat([pre_vote.reshape(2, 1, C, P, 4, 4) for pre_vote in pre_votes], dim=1)\n",
    "# out = out.reshape(2, C, P, 4, 4)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch split channel\n",
    "x = torch.randn(2, 8, 16, 4, 4)\n",
    "x = torch.split(x, 1, dim=1)\n",
    "print(len(x), x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.Routing import CapsuleRouting\n",
    "import torch\n",
    "routing = {\n",
    "            \"type\": \"dynamic\",\n",
    "            \"iters\": 3,\n",
    "            \"temp\": 1.5,\n",
    "        }\n",
    "routinglayer = CapsuleRouting(B=8, C=10, P=4, cap_style='c', routing=routing)\n",
    "x = torch.randn(2, 8, 16, 4, 4)\n",
    "a = torch.randn(2, 8, 4, 4)\n",
    "out, a_out = routinglayer(x, a)\n",
    "print(out.shape, a_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on CIFAR10 dataset with ResNet18\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from Capsule.ultis import CIFAR10read, Caltech101read, CIFAR100read\n",
    "import numpy as np\n",
    "\n",
    "dataset = CIFAR100read(mode='train', data_path='data')\n",
    "model = models.resnet18(pretrained=True).cuda()\n",
    "model.avgpool = nn.Identity()\n",
    "model.fc = nn.Identity()\n",
    "model.eval()\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "train_list = []\n",
    "train_label = []\n",
    "\n",
    "for i, (x, y, image) in enumerate(dataloader):\n",
    "    out = model(x.cuda())\n",
    "    out = out.reshape(out.shape[0], -1, 7, 7)\n",
    "    train_list.append(out.cpu().detach().numpy())\n",
    "    train_label.append(y.numpy())\n",
    "\n",
    "train_list = np.concatenate(train_list, axis=0)\n",
    "train_label = np.concatenate(train_label, axis=0)\n",
    "print(train_list.shape)\n",
    "# save train_list and train_label to h5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('data/CIFAR100-features/cifar100_train.h5', 'w') as f:\n",
    "    f.create_dataset('data', data=train_list)\n",
    "    f.create_dataset('label', data=train_label)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import h5py\n",
    "\n",
    "hf = h5py.File('data/CIFAR100-features/cifar100_train.h5', 'r')\n",
    "train_data = np.array(hf['data'])\n",
    "train_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "hf = h5py.File('data/CIFAR100-features/cifar100_val.h5', 'r')\n",
    "test_data = np.array(hf['data'])\n",
    "test_label = np.array(hf['label'])\n",
    "hf.close()\n",
    "\n",
    "train_data = np.mean(train_data, axis=(2, 3))\n",
    "test_data = np.mean(test_data, axis=(2, 3))\n",
    "\n",
    "#scikit-learn MLPClassifier on train_data, train_label\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(512, 512), max_iter=1000)\n",
    "clf.fit(train_data, train_label)\n",
    "accuracy_score(clf.predict(test_data), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(train_data, train_label)\n",
    "accuracy_score(clf.predict(train_data), train_label)\n",
    "accuracy_score(clf.predict(test_data), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Capsule.ultis import Caltech101read\n",
    "import numpy as np\n",
    "dataset = Caltech101read(data_path='data', mode='train', transform=None)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 16, 4, 4]) torch.Size([2, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "from CapsuleLayer import CapsulePooling\n",
    "import torch\n",
    "\n",
    "v = torch.rand(2, 8, 16, 7, 7)\n",
    "a = torch.rand(2, 8, 7, 7)\n",
    "pooling = CapsulePooling(size=(4, 4))\n",
    "v, a = pooling(v, a)\n",
    "print(v.shape, a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "maxpool = torch.nn.AdaptiveMaxPool2d((1, 1), return_indices=True)\n",
    "v = torch.randn(2, 8, 16, 7, 7)\n",
    "a = torch.randn(2, 8, 7, 7)\n",
    "a, indices = maxpool(a)\n",
    "indices_tile = torch.unsqueeze(indices, 2).expand(-1, -1, 16, -1, -1)\n",
    "indices_tile= indices_tile.reshape(2, 8 * 16, -1)\n",
    "v_flatten = v.reshape(2, 8 * 16, -1)\n",
    "v = torch.gather(v_flatten, 2, indices_tile)\n",
    "v_out = v.reshape(2, 8, 16, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-1.6286]],\n",
       "\n",
       "          [[-0.8624]],\n",
       "\n",
       "          [[-0.4301]],\n",
       "\n",
       "          [[-0.0058]],\n",
       "\n",
       "          [[ 2.1029]],\n",
       "\n",
       "          [[-0.4758]],\n",
       "\n",
       "          [[-0.9780]],\n",
       "\n",
       "          [[ 1.4214]],\n",
       "\n",
       "          [[ 1.6565]],\n",
       "\n",
       "          [[-0.7233]],\n",
       "\n",
       "          [[-1.2424]],\n",
       "\n",
       "          [[ 0.5341]],\n",
       "\n",
       "          [[-0.5042]],\n",
       "\n",
       "          [[-1.4775]],\n",
       "\n",
       "          [[-2.9094]],\n",
       "\n",
       "          [[-0.7909]]],\n",
       "\n",
       "\n",
       "         [[[-0.3428]],\n",
       "\n",
       "          [[ 0.0092]],\n",
       "\n",
       "          [[ 0.1588]],\n",
       "\n",
       "          [[-0.0689]],\n",
       "\n",
       "          [[-1.2629]],\n",
       "\n",
       "          [[ 1.1913]],\n",
       "\n",
       "          [[-1.7891]],\n",
       "\n",
       "          [[-1.2167]],\n",
       "\n",
       "          [[ 1.4765]],\n",
       "\n",
       "          [[ 0.4728]],\n",
       "\n",
       "          [[-0.2214]],\n",
       "\n",
       "          [[ 0.7420]],\n",
       "\n",
       "          [[-0.9598]],\n",
       "\n",
       "          [[ 0.2020]],\n",
       "\n",
       "          [[-1.5140]],\n",
       "\n",
       "          [[-0.2723]]],\n",
       "\n",
       "\n",
       "         [[[ 0.6037]],\n",
       "\n",
       "          [[ 0.7285]],\n",
       "\n",
       "          [[-0.1136]],\n",
       "\n",
       "          [[ 1.9045]],\n",
       "\n",
       "          [[-0.7565]],\n",
       "\n",
       "          [[-0.0130]],\n",
       "\n",
       "          [[-0.3235]],\n",
       "\n",
       "          [[ 1.7284]],\n",
       "\n",
       "          [[-1.2155]],\n",
       "\n",
       "          [[ 0.9181]],\n",
       "\n",
       "          [[-0.6784]],\n",
       "\n",
       "          [[ 0.3096]],\n",
       "\n",
       "          [[ 2.2318]],\n",
       "\n",
       "          [[ 1.7858]],\n",
       "\n",
       "          [[ 1.3160]],\n",
       "\n",
       "          [[ 0.6429]]],\n",
       "\n",
       "\n",
       "         [[[ 0.7561]],\n",
       "\n",
       "          [[ 0.7470]],\n",
       "\n",
       "          [[-1.0233]],\n",
       "\n",
       "          [[-0.4337]],\n",
       "\n",
       "          [[ 0.6377]],\n",
       "\n",
       "          [[-1.0869]],\n",
       "\n",
       "          [[ 1.5416]],\n",
       "\n",
       "          [[-0.2663]],\n",
       "\n",
       "          [[ 0.4536]],\n",
       "\n",
       "          [[-0.0668]],\n",
       "\n",
       "          [[-1.4005]],\n",
       "\n",
       "          [[-0.6260]],\n",
       "\n",
       "          [[-0.9638]],\n",
       "\n",
       "          [[-0.6034]],\n",
       "\n",
       "          [[ 1.0704]],\n",
       "\n",
       "          [[-1.4134]]],\n",
       "\n",
       "\n",
       "         [[[-0.8678]],\n",
       "\n",
       "          [[ 1.0855]],\n",
       "\n",
       "          [[-0.4375]],\n",
       "\n",
       "          [[ 1.7229]],\n",
       "\n",
       "          [[ 0.1591]],\n",
       "\n",
       "          [[-0.2192]],\n",
       "\n",
       "          [[-0.1382]],\n",
       "\n",
       "          [[ 0.4720]],\n",
       "\n",
       "          [[-0.2758]],\n",
       "\n",
       "          [[ 0.3859]],\n",
       "\n",
       "          [[ 0.6566]],\n",
       "\n",
       "          [[-1.0568]],\n",
       "\n",
       "          [[ 0.3298]],\n",
       "\n",
       "          [[-0.5586]],\n",
       "\n",
       "          [[ 0.7505]],\n",
       "\n",
       "          [[ 0.4725]]],\n",
       "\n",
       "\n",
       "         [[[-1.6169]],\n",
       "\n",
       "          [[ 0.8366]],\n",
       "\n",
       "          [[ 0.9287]],\n",
       "\n",
       "          [[ 0.4853]],\n",
       "\n",
       "          [[ 0.9467]],\n",
       "\n",
       "          [[ 1.1550]],\n",
       "\n",
       "          [[-0.8043]],\n",
       "\n",
       "          [[-1.0068]],\n",
       "\n",
       "          [[ 0.5917]],\n",
       "\n",
       "          [[-0.3522]],\n",
       "\n",
       "          [[-1.0884]],\n",
       "\n",
       "          [[-0.1948]],\n",
       "\n",
       "          [[ 1.5069]],\n",
       "\n",
       "          [[-0.8309]],\n",
       "\n",
       "          [[ 0.5518]],\n",
       "\n",
       "          [[ 1.3103]]],\n",
       "\n",
       "\n",
       "         [[[ 0.1065]],\n",
       "\n",
       "          [[ 0.1295]],\n",
       "\n",
       "          [[-0.9592]],\n",
       "\n",
       "          [[-1.4318]],\n",
       "\n",
       "          [[-0.7060]],\n",
       "\n",
       "          [[-0.6534]],\n",
       "\n",
       "          [[ 0.5911]],\n",
       "\n",
       "          [[-1.2578]],\n",
       "\n",
       "          [[-1.3044]],\n",
       "\n",
       "          [[ 0.3637]],\n",
       "\n",
       "          [[-0.6631]],\n",
       "\n",
       "          [[ 2.0935]],\n",
       "\n",
       "          [[ 0.5930]],\n",
       "\n",
       "          [[ 1.8449]],\n",
       "\n",
       "          [[ 1.7838]],\n",
       "\n",
       "          [[ 0.3043]]],\n",
       "\n",
       "\n",
       "         [[[ 0.5626]],\n",
       "\n",
       "          [[ 1.9606]],\n",
       "\n",
       "          [[-0.1105]],\n",
       "\n",
       "          [[-0.2115]],\n",
       "\n",
       "          [[ 0.0971]],\n",
       "\n",
       "          [[ 1.7690]],\n",
       "\n",
       "          [[ 0.6052]],\n",
       "\n",
       "          [[-0.4486]],\n",
       "\n",
       "          [[-1.1772]],\n",
       "\n",
       "          [[ 0.3668]],\n",
       "\n",
       "          [[-0.4232]],\n",
       "\n",
       "          [[-1.4473]],\n",
       "\n",
       "          [[-1.6063]],\n",
       "\n",
       "          [[ 1.4668]],\n",
       "\n",
       "          [[-1.1752]],\n",
       "\n",
       "          [[ 0.5670]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 1.0547]],\n",
       "\n",
       "          [[ 0.0468]],\n",
       "\n",
       "          [[ 2.0059]],\n",
       "\n",
       "          [[ 0.2210]],\n",
       "\n",
       "          [[-0.4637]],\n",
       "\n",
       "          [[-1.4630]],\n",
       "\n",
       "          [[-1.2324]],\n",
       "\n",
       "          [[ 0.2445]],\n",
       "\n",
       "          [[ 0.4856]],\n",
       "\n",
       "          [[-2.0734]],\n",
       "\n",
       "          [[ 0.0261]],\n",
       "\n",
       "          [[ 0.3029]],\n",
       "\n",
       "          [[ 0.5164]],\n",
       "\n",
       "          [[-1.3104]],\n",
       "\n",
       "          [[ 0.3229]],\n",
       "\n",
       "          [[ 2.0211]]],\n",
       "\n",
       "\n",
       "         [[[-0.2798]],\n",
       "\n",
       "          [[-0.5513]],\n",
       "\n",
       "          [[-1.2584]],\n",
       "\n",
       "          [[ 0.6439]],\n",
       "\n",
       "          [[ 1.2850]],\n",
       "\n",
       "          [[ 0.0866]],\n",
       "\n",
       "          [[-0.3025]],\n",
       "\n",
       "          [[-0.2625]],\n",
       "\n",
       "          [[-0.0229]],\n",
       "\n",
       "          [[ 0.5771]],\n",
       "\n",
       "          [[ 0.0440]],\n",
       "\n",
       "          [[-0.3656]],\n",
       "\n",
       "          [[ 0.9618]],\n",
       "\n",
       "          [[ 0.3102]],\n",
       "\n",
       "          [[-0.9364]],\n",
       "\n",
       "          [[-0.0035]]],\n",
       "\n",
       "\n",
       "         [[[-1.0197]],\n",
       "\n",
       "          [[ 0.1032]],\n",
       "\n",
       "          [[-0.8195]],\n",
       "\n",
       "          [[ 0.8694]],\n",
       "\n",
       "          [[-1.2504]],\n",
       "\n",
       "          [[-0.8133]],\n",
       "\n",
       "          [[-1.0253]],\n",
       "\n",
       "          [[-0.2222]],\n",
       "\n",
       "          [[-0.3824]],\n",
       "\n",
       "          [[ 1.1606]],\n",
       "\n",
       "          [[-1.1581]],\n",
       "\n",
       "          [[ 0.2726]],\n",
       "\n",
       "          [[ 1.9606]],\n",
       "\n",
       "          [[-1.0356]],\n",
       "\n",
       "          [[-1.9012]],\n",
       "\n",
       "          [[-0.5037]]],\n",
       "\n",
       "\n",
       "         [[[ 0.1482]],\n",
       "\n",
       "          [[ 0.1999]],\n",
       "\n",
       "          [[ 1.7074]],\n",
       "\n",
       "          [[-0.4102]],\n",
       "\n",
       "          [[ 1.4306]],\n",
       "\n",
       "          [[-0.5000]],\n",
       "\n",
       "          [[-1.5587]],\n",
       "\n",
       "          [[-0.6078]],\n",
       "\n",
       "          [[ 2.4137]],\n",
       "\n",
       "          [[-1.4530]],\n",
       "\n",
       "          [[ 0.4275]],\n",
       "\n",
       "          [[ 0.9715]],\n",
       "\n",
       "          [[ 0.9371]],\n",
       "\n",
       "          [[-1.4718]],\n",
       "\n",
       "          [[ 0.6808]],\n",
       "\n",
       "          [[ 0.0799]]],\n",
       "\n",
       "\n",
       "         [[[ 0.5696]],\n",
       "\n",
       "          [[-0.0673]],\n",
       "\n",
       "          [[-0.0853]],\n",
       "\n",
       "          [[ 0.5229]],\n",
       "\n",
       "          [[-1.0872]],\n",
       "\n",
       "          [[-0.6057]],\n",
       "\n",
       "          [[ 0.6039]],\n",
       "\n",
       "          [[ 0.6498]],\n",
       "\n",
       "          [[ 0.4699]],\n",
       "\n",
       "          [[ 0.2969]],\n",
       "\n",
       "          [[-1.0979]],\n",
       "\n",
       "          [[ 0.2829]],\n",
       "\n",
       "          [[ 0.6341]],\n",
       "\n",
       "          [[-1.1080]],\n",
       "\n",
       "          [[ 0.4186]],\n",
       "\n",
       "          [[ 0.3796]]],\n",
       "\n",
       "\n",
       "         [[[ 0.9756]],\n",
       "\n",
       "          [[-0.4139]],\n",
       "\n",
       "          [[ 0.5012]],\n",
       "\n",
       "          [[-0.1376]],\n",
       "\n",
       "          [[ 1.0271]],\n",
       "\n",
       "          [[ 0.5482]],\n",
       "\n",
       "          [[ 0.6831]],\n",
       "\n",
       "          [[ 0.0370]],\n",
       "\n",
       "          [[ 1.2609]],\n",
       "\n",
       "          [[ 0.6786]],\n",
       "\n",
       "          [[ 0.7226]],\n",
       "\n",
       "          [[-0.4031]],\n",
       "\n",
       "          [[-0.3302]],\n",
       "\n",
       "          [[ 0.3770]],\n",
       "\n",
       "          [[ 1.0635]],\n",
       "\n",
       "          [[-1.0735]]],\n",
       "\n",
       "\n",
       "         [[[-2.1349]],\n",
       "\n",
       "          [[-1.6654]],\n",
       "\n",
       "          [[-0.0067]],\n",
       "\n",
       "          [[-0.9724]],\n",
       "\n",
       "          [[ 0.1382]],\n",
       "\n",
       "          [[-0.1048]],\n",
       "\n",
       "          [[-0.7651]],\n",
       "\n",
       "          [[-0.5147]],\n",
       "\n",
       "          [[-0.8172]],\n",
       "\n",
       "          [[ 1.4499]],\n",
       "\n",
       "          [[ 1.0035]],\n",
       "\n",
       "          [[-0.4618]],\n",
       "\n",
       "          [[ 0.8063]],\n",
       "\n",
       "          [[ 1.8104]],\n",
       "\n",
       "          [[ 0.4243]],\n",
       "\n",
       "          [[ 0.3278]]],\n",
       "\n",
       "\n",
       "         [[[-0.7960]],\n",
       "\n",
       "          [[ 0.2876]],\n",
       "\n",
       "          [[ 2.1061]],\n",
       "\n",
       "          [[ 2.0437]],\n",
       "\n",
       "          [[-0.8416]],\n",
       "\n",
       "          [[-0.4480]],\n",
       "\n",
       "          [[-1.1525]],\n",
       "\n",
       "          [[ 0.3824]],\n",
       "\n",
       "          [[ 1.5223]],\n",
       "\n",
       "          [[ 1.9046]],\n",
       "\n",
       "          [[-1.0043]],\n",
       "\n",
       "          [[-2.8085]],\n",
       "\n",
       "          [[-0.9085]],\n",
       "\n",
       "          [[-2.3319]],\n",
       "\n",
       "          [[ 0.4108]],\n",
       "\n",
       "          [[ 0.0581]]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capsule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
